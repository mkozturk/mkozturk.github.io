{
  "hash": "c7ededa971cfbed83e78f1dd041cef77",
  "result": {
    "markdown": "---\ntitle: \"Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak\"\ndescription: \"Algoritmalarınızı scikit-learn yapısına uygun olarak yazın ve bu zengin kütüphanenin gücünü tam olarak kullanın\"\nauthor: \"Kaan Öztürk\"\ndate: \"11/25/2023\"\ntoc: true\nimage: scikit-learn-logo.png\nformat: \n  html:\n    code-fold: false\ncategories:\n    - data science\n---\n\nPython'la yapay öğrenme modelleri çalıştıran her veri bilimcinin aşina olduğu bir kütüphanedir [`scikit-learn`](https://scikit-learn.org/stable/). Son derece zengin bir algoritma koleksiyonunu barındırır. Çoğu zaman o algoritmaları olduğu gibi kullanmak yeterlidir.\n\nAma bazen çok özelleşmiş bir algoritmaya ihtiyaç duyabilirsiniz. Bunu `scikit-learn` arayüzüne uygun şekilde yazarsanız, kütüphanenin sağladığı birçok başka kolaylığa da rahatça erişebilirsiniz.\n\nBu yazıda, kendi regresyon veya sınıflandırıcı modelinizi nasıl `scikit-learn` modülünün parçasıymış gibi yazabileceğinizi anlatacağım. Önce, `scikit-learn` sisteminin nasıl kullanıldığına bir bakalım.\n\n# `scikit-learn` arayüzü\n\nAşina olduğumuz `scikit-learn` yapısında üç aşama vardır: Tahminleyici nesnesini yaratmak, veriyle eğitmek, sonra eğitilmiş modelden tahmin üretmek. Mesela:\n\n```python\nknn = KNeighborsClassifier()  # (1)\nknn.fit(X_train, y_train)     # (2)\ny_pred = knn.predict(X_test)  # (3)\n```\n\n1. `scikit-learn` içindeki her algoritma bir *estimator* nesne sınıfı olarak kodlanmıştır. Regresyon, sınıflayıcı ve diğer tahminleyici türlerinin hepsi temel `BaseEstimator` sınıfından türetilmiştir. \n2. Eğitme işi `fit()` metodu ile sağlanır. Buradaki eğitim, sözgelişi, regresyon parametrelerini belirlemek, PCA bileşenlerini hesaplamak, veya bir ölçekleme yapmakta kullanılacak parametreleri bulmak olabilir.\n3. Eğitilen modeli kullanarak yeni veri ile kestirim yapmak için `predict()` metodu kullanılır. \n\nBunlar en temel işlemler. Bunların yanı sıra, sınıflayıcı (classifier) tahminleyicilerde `predict_proba()` veye `decision_function()` metodları bulunabilir. Bunlar her bir tahmin sınıfı için evet/hayır cevabı yerine daha \"yumuşak\" puanlar sağlarlar. Ayrıca regresyon tahminleyicilerinde `score()` gibi başarı ölçüsü veren metotlar olabilir.\n\nBazı algoritmalar ise veriyi dönüştürme amaçlı kullanılırlar. Örneğin ölçekleme veya PCA işlemleri. Bu tür tahminleyicilerde `transform()` metodu bulunur.\n\n```Python\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_trf = scaler.transform(X_train)\n```\n\nBir çok durumda birkaç tahminleyici ve dönüştürücü birbirlerinin ardı sıra eklenerek kullanılırlar. `scikit-learn` bunu otomatikleştirmek için bir `Pipeline` (veri hattı) nesnesi sağlar. Bir `Pipeline` nesnesinin kendine ait `fit()` ve `predict()` metodları bulunur, böylece kendi başına bir tahminleyici imiş gibi kullanılabilir.\n\nÖrneğin, aşağıdaki adımlar, verinin önce ölçeklenip sonra k-NN ile sınıflandırıldığı bir işlem sağlar.\n\n```Python\npipe = Pipeline(steps=[('scale', StandardScaler), ('knn', knn)])\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\n```\nYeni bir tahminleyici yazacağımız zaman, burada özetlenen arayüzle tutarlı olmasını isteriz. `scikit-learn` bize bunun için bir standart ve ona uymak için kullanılacak araçlar sunuyor.\n\n::: {.callout-note title=\"Resmi belgeler\"}\nBuradaki örnekler ve açıklamalar kaçınılmaz olarak eksik kalacak. Tam bir başvuru kaynağı olarak resmi dökümantasyona ([Developing scikit-learn estimators](https://scikit-learn.org/stable/developers/develop.html)) bakabilirsiniz.\n:::\n\n# Neden?\n\nNeden tahminleyici algoritmamızı `scikit-learn` yapısına uydurmak için uğraşalım? Eğitim ve kestirim adımlarını iki ayrı fonksiyon halinde tutmakla, veya tek bir nesne sınıfı yaratıp içine `fit()` ve `predict()` metotlarını koymakla niye yetinmeyelim? Haklı sorular.\n\n`scikit-learn` standardına uymanın en bariz yararı, kütüphanenin sunduğu üst seviye araçları kolayca kullanabilme imkânı. Örneğin, algoritmanız önceden verilerin ölçeklenmesini gerektiriyorsa, bunu kendiniz kodlamak zorunda kalmazsınız. `scikit-learn` içinde bulunan dönüştürücü ile beraber bir pipeline kurabilirsiniz.\n\nKeza `scikit-learn` içindeki çapraz doğrulama (cross-validation), parametre uzayı tarama (grid search) gibi işlemleri de kolayca yapabilirsiniz. Otomatik model seçme işlemlerine kendi algoritmanızı pürüzsüz bir şekilde entegre edebilirsiniz.\n\nÜstelik, içinde `fit()` ve `predict()` metotları bulunan bir nesne sınıfı yaptıktan sonra, bunu `scikit-learn` standardına uygun hale getirmenin zahmetsiz bir iş olduğunu göreceğiz. Yani az bir çabayla, büyük bir güç elde ediyoruz.\n\n::: {.callout-tip title=\"Kod kalıpları\"}\nYeni tahminleyiciler geliştirirken örnek alabileceğiniz [kod kalıpları mevcut](https://github.com/scikit-learn-contrib/project-template). Sıfırdan başlamak yerine uygun bir kalıbı alıp içini uygun şekilde doldurabilirsiniz.\n:::\n\n::: {.callout-warning}\n# Tekerleği baştan icat etmeyin\nİstediğiniz dönüşümü yapabilen bir `scikit-learn` fonksiyonu varsa onu kullanın. Kütüphanenin dökümantasyonuna göz gezdirin, aşina olun.\n:::\n\n# Kestirim\n\nİlk örnek olarak, regresyon kestirimi yapan bir sınıf yaratalım. Algoritmik ayrıntılara boğulmamak için basit tutalım: Tahmin olarak eğitim kümesinin ortalamasını veya ortancasını veren bir kestirici olsun.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.validation import check_is_fitted\n\nclass SimpleRegressor(BaseEstimator, RegressorMixin):   # (1)\n    def __init__(self, method=\"mean\"):                  # (2)\n        self.method = method\n        \n    def fit(self, X, y):                                # (3)\n        if self.method not in [\"mean\", \"median\"]:       # (4)\n            raise ValueError(\"method must be 'mean' or 'median'\")\n        X, y = check_X_y(X, y)\n        if self.method==\"mean\":\n            self.coef_ = np.mean(y)                     \n        if self.method==\"median\":\n            self.coef_ = np.median(y)                   # (5)\n        self.n_features_in_ = X.shape[1]\n        return self                                     # (6)\n    \n    def predict(self, X):\n        X = check_array(X)                              # (7)\n        check_is_fitted(self, attributes=[\"coef_\"])\n        return np.ones(X.shape[0])*self.coef_           # (8)\n```\n:::\n\n\nBunu satır satır açıklayalım.\n\n**(1) Miras alınan sınıflar**  \nKestirici sınıfımıza `SimpleRegressor` adını veriyoruz. Bu sınıf `BaseEstimator` sınıfında tanımlanmış elemanları miras alıyor. Ayrıca `RegressorMixin` sınıfını ebeveyn olarak ekliyoruz. Böylece hem bunun bir regresyon kestiricisi olarak tanınmasını, hem de `score()` metodunun miras alınmasını sağlıyor.\n\n**(2) Başlatıcı**   \nSınıf inşacısının (constructor) basit olması istenir. Kestiriciyi çağırırken, algoritmayla ilgili hiperparametreleri burada veririz. \n\nDikkat edilecek noktalar:\n\n* Her parametre, kendisiyle tıpatıp aynı isimde bir nesne değişkenine dönüştürülmelidir (`method` ve `self.method` gibi). Tahminleyicilerin `get_param()` ve `set_param()` metodlarının doğru çalışması buna bağlıdır.\n* Bütün parametrelerin varsayılan (default) değerleri olmalıdır. Bir tahminleyici hiç bir parametre verilmeden de makul bir şekilde çalışabilmeli. Örneğin `reg = SimpleRegressor()`.\n* Veri denetlemesi veya hata düzeltmesi gibi işlemler sadece `fit()` içinde yapılmalıdır. `__init__` içinde atama dışında bir işlem bulunmamalıdır. Aksi takdirde, hiperparametre ataması için kullanılan `set_param()` gibi alternatif yollar (mesela grid search işleminde) doğru çalışmaz.\n\n**(3) fit() metodu**  \nBu metodun her zaman `X` (veri matrisi) ve `y` (hedef değer vektörü) alması gerekir. \n\n::: {.callout-note }\nGözetimsiz (unsupervised) algoritmalarda `y` alınması gerekmez, yine de API tutarlılığı açısından parametre listesinde bulunur,  varsayılan değer olarak `None` atanır: `def fit(self, X, y=None)`.\n:::\n\n**(4) Doğrulama kontrolleri**  \nHiperparametrelerin doğru aralıkta olması, verilerin uygun biçimde olması gibi testler burada yapılmalıdır. Bunun için `sklearn.utils` modülünde çeşitli fonksiyonlar mevcut. Burada `check_X_y` ile verilerin uygun biçim ve boyutta olduğunu kontrol ediyoruz.\n\n::: {.callout-tip}\n# Numpy kullanın\n`scikit-learn` algoritmaları pandas veri tabloları alabilir, ama bunları kendi içinde Numpy dizilerine dönüştürür. Siz de tahminleyicilerinizi kodlarken sadece Numpy yapılarını kullanın.\n:::\n\n**(5) Model parametreleri** \nSeçilen yönteme göre, hedef vektörü `y`'nin ortalama veya ortancasını hesaplayıp, `coef_` isimli bir nesne değişkeninde tutuyoruz. Bu isim istediğiniz gibi seçilebilir. Birçok regresyon tahmincisi `coef_` kullandığı için biz de burada aynısını kullandık.\n\nVeri kullanılarak hesaplanan değişkenlerin alt çizgiyle bitmesi gerekir. \n\n`n_features_in_` değişkeni veride kaç öznitelik olduğunun hesabını tutmamıza yarar. Kestirme için kullanılan verinin de aynı sayıda özniteliği olduğunu kontrol etmek için kullanılır.\n\n**(6) `fit()` bir tahminleyici döndürür**  \n`fit()` metodu her zaman `self` döndürmelidir. Böylece \n\n`y_pred = SimpleRegressor().fit(X_train, y_train).predict(X_test)` \n\ngibi metot zincirleri kurmak mümkün olur.\n\n**(7) Kestirim aşaması kontrolleri**  \n`check_array()` fonksiyonu girdinin uygun biçimde bir veri yapısı olup olmadığını denetler.\n\nTahminleyicinin `fit()` metodu bir kere çalıştırılıp model eğitilmeden kestirim yapmak anlamsız olacaktır. `check_is_fitted()` fonksiyonuyla bunun denetimini yaparız. Fonksiyon buradaki haliyle `coef_` değişkeninin mevcut olup olmadığına bakar. `attributes` parametresini kullanmasaydık, altçizgi (`_`) ile biten herhangi bir değişken olup olmadığını kontrol ederdi.\n\n**(8) `predict()` bir dizi (array) döndürür**  \nModel tahmini için gerekli işlemler yapıldıktan sonra, test verisindeki satır sayısı uzunluğunda bir boyutlu bir dizi (array) elde ederiz.\n\n::: {.callout-tip}\n# Genel amaçlı tahminleyiciler yazın\nÖzel bir probleme odaklanıyor olsanız da, çözümün işe yarar en genel halinin ne olabileceğini düşünün ve tahminleyicinizi ona göre yazın. Tahminleyicinizi bambaşka bir bağlam ve problemde de kullanılabilecek şekilde tasarlayın. Yazdığınız kod, veri hakkında çok fazla varsayıma bağlı olmasın. Veri biçimlendirme işlemlerini tahminleyiciye koymayın. Ya önceden yapın, ya da veri akış hattının başına ayrı bir işlem olarak koyun.\n:::\n\nŞimdi bu kestiriciyi diğer `scikit-learn` kestiricileri gibi kullanabiliriz. Önce rastgele veri üretelim:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nnp.random.seed(29101923)\nX_train, X_test = np.random.rand(10,2), np.random.rand(5,2)\ny_train, y_test = np.random.randn(10), np.random.randn(5)\ny_train.mean(), np.median(y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```\n(0.3662107108568314, 0.2861502525347002)\n```\n:::\n:::\n\n\nBasit regresyon tahminleyicimiz, eğitim kümesinin hedef değişkeninin ortalaması hesaplayacak ve her yeni kestirim için aynı değeri verecek:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nSimpleRegressor().fit(X_train, y_train).predict(X_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([0.36621071, 0.36621071, 0.36621071, 0.36621071, 0.36621071])\n```\n:::\n:::\n\n\nTahminleyicinin ortanca değer kullanmasını istersek:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nSimpleRegressor(method=\"median\").fit(X_train, y_train).predict(X_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\narray([0.28615025, 0.28615025, 0.28615025, 0.28615025, 0.28615025])\n```\n:::\n:::\n\n\n`RegressorMixin` sınıfından miras alınan `score` metodunu kullanarak, kestirimimizin $R^2$ puanını hesaplatabiliriz.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nSimpleRegressor().fit(X_train, y_train).score(X_test,y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n-0.6810225522666027\n```\n:::\n:::\n\n\n# Standartlara uyum kontrolü\n\nYazdığımız tahminleyicinin `scikit-learn` standartlarına uyup uymadığını kontrol etmek için `utils.estimator_checks.check_estimator` fonksiyonunu kullanırız.\n\n`check_estimator`, yaptığı kontrollerin yanı sıra, mevcut veri kümelerinde tahminleyicimizin ne kadar isabetli olduğunu da test eder. Ancak, mecvut haliyle kestiricimiz testi geçemiyor:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.utils.estimator_checks import check_estimator\ncheck_estimator(SimpleRegressor())\n```\n:::\n\n\n```\nAssertionError                            Traceback (most recent call last)\n...\n   2277     if not regressor._get_tags()[\"poor_score\"]:\n-> 2278         assert regressor.score(X, y_) > 0.5\n   2279 \n   2280 \n\nAssertionError: \n```\n\nBunun nedeni, çok basit bir kestirici yaratmış olmamız ve hatasının yüksek oluşu.\n\nKötü tahmin yapan bir kestiriciyi `scikit-learn`'e kabul ettirmek mümkün. Bunun için *estimator tag* özelliklerini kullanırız. Bu etiketlerin ne olduklarının açıklaması ve tam bir listesi için [belgelere bakabilirsiniz](https://scikit-learn.org/stable/developers/develop.html#estimator-tags). Buradaki amacımız için *poor_score* etiketini `True` değeriyle kestiricimize eklemeliyiz. Bunun için kestirici sınıfına `_more_tags` isimli bir metod eklemek gerekli.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.utils.estimator_checks import check_estimator\ndef _more_tags(self):\n    return {'poor_score': True}\nSimpleRegressor._more_tags = _more_tags\ncheck_estimator(SimpleRegressor())\n```\n:::\n\n\nŞimdi hiç bir hata mesajı almadık. Tahminleyicimiz testleri geçti.\n\n# Sınıflandırma\n\nŞimdi de basit bir sınıflandırıcı (classifier) yaratalım. Her nokta için tahmin edilen sııf, ona en yakın komşusunun sınıfı olsun (1-NN algoritması).\n\nProje kalıpları içeren [project-template](https://github.com/scikit-learn-contrib/project-template) reposunda tam da bu işi yapan bir kod kalıbı mevcut. Küçük değişikliklerle kullanalım:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code code-fold=\"show\"}\nimport numpy as np\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.utils import check_X_y, check_array\nfrom sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\nfrom sklearn.metrics import euclidean_distances\n\nclass TemplateClassifier(BaseEstimator,ClassifierMixin):\n    \"\"\" An example classifier which implements a 1-NN algorithm.\n    For more information regarding how to build your own classifier, read more\n    in the User Guide.\n    \n    Parameters\n    ----------\n    demo_param : str, default='demo'\n        A parameter used for demonstation of how to pass and store parameters.\n        \n    Attributes\n    ----------\n    X_ : ndarray, shape (n_samples, n_features)\n        The input passed during fit().\n    y_ : ndarray, shape (n_samples,)\n        The labels passed during fit().\n    classes_ : ndarray, shape (n_classes,)\n        The classes seen at fit().\n        \n    Examples\n    --------\n    >>> X = [[0, 0], [1, 1]]\n    >>> y = [0, 1]\n    >>> clf = TemplateClassifier()\n    >>> clf.fit(X, y)\n\n    >>> rng = np.random.RandomState(13)\n    >>> X_test = [[0.2,0.2], [0.4,0.4], [0.6, 0.6], [0.8, 0.8]]\n    >>> clf.predict(X_test)\n    array([0, 0, 1, 1])\n    \"\"\"\n    def __init__(self, demo_param='demo'):\n        self.demo_param = demo_param\n\n    def fit(self, X, y):\n        \"\"\"A reference implementation of a fitting function for a classifier.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The training input samples.\n        y : array-like, shape (n_samples,)\n            The target values. An array of int.\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        # Check that X and y have correct shape\n        X, y = check_X_y(X, y)\n        \n        # Store the number of features\n        self.n_features_in_ = X.shape[1]\n        \n        # Store the classes seen during fit\n        self.classes_ = unique_labels(y)\n\n        self.X_ = X\n        self.y_ = y\n        # Return the classifier\n        return self\n\n    def predict(self, X):\n        \"\"\" A reference implementation of a prediction for a classifier.\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            The input samples.\n        Returns\n        -------\n        y : ndarray, shape (n_samples,)\n            The label for each sample is the label of the closest sample\n            seen during fit.multiclass.\n        \"\"\"\n        # Check is fit had been called\n        check_is_fitted(self, ['X_', 'y_'])\n\n        # Input validation\n        X = check_array(X)\n\n        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n        return self.y_[closest]\n```\n:::\n\n\nBu örnek, regresyon için hazırladığımız örnekten daha karmaşık değil. Uzunluğu, daha ayrıntılı açıklamalar içermesinden kaynaklanıyor. Sizin de kendi sınıflayıcılarınızı yazarken, bu örnekte olduğu gibi, değişkenlerin ne tip olduğu ve ne anlama geldiği, nesne değişkenlerinin (attributes) listesi ve açıklaması, tahminleyicinin kullanımına örnekler, ayrıca `fit()` ve `predict()` metodlarının aldığı girdilerin ve döndürdüğü değerlerin açıklamaları gibi bilgileri koymanız gerekir.\n\nSınıflandırıcılarda hedef değerler ayrık kategorilerdir. Bunlar tam sayı veya dize (string) olarak kullanılmalı. Kategori listesi, `classes_` isimli sıralanmış bir dizi olarak nesne değişkeni olarak saklanmalıdır.\n\nVerilerin kategorilerini baştan bilmiyorsanız, `utils.multiclass.unique_labels()` fonksiyonu ile bunları verilerden alabilirsiniz.\n\nBuradaki basit sınıflandırıcı (1-NN) verilen bir tahmin noktasına, eğitim kümesindeki en yakın komşunun değerini atar. Bu yüzden eğitim kümesi kestirim (`predict()`) adımında erişilebilir olmalıdır. Bu yüzden `fit()` içinde `X_` ve `y_` nesne değişkenleri yaratılır ve bunlar daha sonra `predict()` içinde tahmin yapmak için kullanılırlar.\n\nAlgoritmanın özelliğine bağlı olarak, sınıflandırıcıya `decision_function()`, `predict_proba()` ve `predict_log_proba()` metodları da eklenebilir.\n\nSınıflandırıcının testlerini yapalım:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ncheck_estimator(TemplateClassifier())\n```\n:::\n\n\nKullanım örneği:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nX = [[0, 0], [1, 1]]\ny = [0, 1]\nclf = TemplateClassifier()\nclf.fit(X, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TemplateClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TemplateClassifier</label><div class=\"sk-toggleable__content\"><pre>TemplateClassifier()</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nX_test = [[0.2,0.2], [0.4,0.4], [0.6, 0.6], [0.8, 0.8]]\ny_test = [0, 1, 1, 1]\nclf.predict(X_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\narray([0, 0, 1, 1])\n```\n:::\n:::\n\n\nModelin doğruluğunu (accuracy) bulmak için `score()` metodunu kullanabiliriz.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nclf.score(X_test, y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.75\n```\n:::\n:::\n\n\n# Veri dönüşümü\n\n*Dönüştürücü* (transformer) bir veri kümesini alıp başka bir biçime çevirir. Yine bir `fit()` metodu vardır, ama ardından `predict()` yerine `transform()` yapılır.\n\n## Hareketli ortalama\nÖrnek olarak, verilerin hareketli ortalamasını veren bir dönüştürücü hazırlayalım. Böyle bir işlem için bir dönüştürücü yazmak aşırı gelebilir, ama yararlı olacağı durumlar vardır. Sözgelişi, modelin eğitim kümesini hazırlarken bir adımda hareketli ortalama alıyor olabiliriz. Önceden bu işlemi bir dönüştürücü haline getirirsek, bütün eğitim ve kestirim sürecini bir pipeline içine koymamız, çapraz doğrulamaları aksamadan yapmamız mümkün olur.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\n\nclass RollingMean(BaseEstimator, TransformerMixin):\n    def __init__(self, n=15):\n        self.n = n\n    \n    def moving_average_(self, a):\n        return np.convolve(a, np.ones(self.n), \"valid\") / self.n\n    \n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        return self\n    \n    def transform(self, X):\n        check_is_fitted(self, 'n_features_in_')\n        X = check_array(X)\n        # Moving average will have len(X) - n + 1 rows. For consistent size, fill the first n-1 rows with NaN.\n        empty_rows = np.array([np.nan]*(X.shape[1]*(self.n-1))).reshape(self.n-1,X.shape[1])\n        movav = np.apply_along_axis(self.moving_average_, axis=0, arr=X)\n        return np.r_[empty_rows, movav]\n```\n:::\n\n\nBu örnekte `fit()` herhangi bir işlem yapmıyor, sadece girdi için doğruluk denetimi yapıyor ve `n_features_in_` nesne özelliğini yaratıyor. Daha sonra `transform()` çağrıldığında bu değişkenin var olup olmadığı denetleniyor.\n\nÜç değişkenli bir rastgele yürüyüş verisi üretelim:\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nnp.random.seed(29101923)\nX = np.cumsum(np.random.rand(30).reshape(3,-1).T, axis=0)\nX\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\narray([[0.74977745, 0.96371498, 0.32437245],\n       [1.71830174, 1.4121694 , 0.50916844],\n       [2.10912041, 1.54142327, 0.77301929],\n       [2.12452587, 1.97102509, 1.23124119],\n       [2.34991983, 2.00229673, 1.87464155],\n       [2.75549879, 2.71351979, 2.71059991],\n       [2.87756103, 3.29451338, 3.27770838],\n       [3.64506605, 3.47371139, 3.9280419 ],\n       [4.18478839, 4.08826894, 4.22898858],\n       [5.0129616 , 4.10754193, 4.32410028]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nRollingMean(n=3).fit(X).transform(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])\n```\n:::\n:::\n\n\nBu örnekte `fit()` ayrı bir iş yapmadığı için, aynı işlem doğrudan `fit_transform()` ile de yapılabilir. Bu metot `TransformerMixin` sınıfından miras alınır, o yüzden bizim açıkça eklememize lüzum yok.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nRollingMean(n=3).fit_transform(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])\n```\n:::\n:::\n\n\n## Minimum-maksimum ölçekleme\nBaşka bir örnek olarak, minimum-maksimum arası ölçekleme için bir dönüştürücü oluşturalım^[Bunun için hazır bir dönüştürücü var ama örnek için yokmuş gibi yapalım.]. Bu örnekte `fit()` metodu boş durmuyor, eğitim kümesinin en büyük ve en küçük değerlerini bulup bir kenara yazıyor.\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nclass MinmaxScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        self.max_ = X.max(axis=0)\n        self.min_ = X.min(axis=0)\n        return self\n    def transform(self, X):\n        check_is_fitted(self, \"max_\")\n        X = check_array(X)\n        return (X - self.min_) / (self.max_ - self.min_)\n```\n:::\n\n\nBurada ölçekleme için herhangi bir parametre alınmadığından başlatıcı `__init__` boş kalıyor (ama Python sentaksı gereği sınıf tanımında bulunmak zorunda).\n\nBu dönüştürücünün bir öncekinden farkı, eğitim kümesine bağlı oluşu. Bu ölçekleyici, eğitim verisinin sütunlarının minimum ve maksimum değerlerini belirliyor. `transform()` işleminde ise, aldığı verileri minimum değer 0 ve maksimum değer 1 olacak şekilde lineer bir fonksiyonla dönüştürüyor.\n\nBu dönüştürücüyü yine rastgele üretilmiş verilerle deneyelim:\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nX_train = np.random.rand(20,3)\nX_test = np.random.rand(10,3)\nX_test\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\narray([[0.35628925, 0.66850959, 0.84267923],\n       [0.69403501, 0.38350617, 0.62806853],\n       [0.38789683, 0.53202186, 0.61269947],\n       [0.51598942, 0.37764728, 0.36217848],\n       [0.02539575, 0.26381172, 0.64366218],\n       [0.91755454, 0.36221054, 0.50155528],\n       [0.09683517, 0.71776499, 0.7394225 ],\n       [0.81163167, 0.49544736, 0.63947337],\n       [0.58945327, 0.50590908, 0.03767375],\n       [0.09311029, 0.12597857, 0.72295531]])\n```\n:::\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nscaler = MinmaxScaler().fit(X_train)\nscaler.transform(X_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\narray([[ 3.42904840e-01,  6.06674521e-01,  8.79083858e-01],\n       [ 7.17503869e-01,  2.46825035e-01,  6.44693329e-01],\n       [ 3.77961291e-01,  4.34343120e-01,  6.27907771e-01],\n       [ 5.20030762e-01,  2.39427514e-01,  3.54297244e-01],\n       [-2.40942497e-02,  9.56970773e-02,  6.61724192e-01],\n       [ 9.65412838e-01,  2.19936867e-01,  5.06519853e-01],\n       [ 5.51403047e-02,  6.68865111e-01,  7.66310361e-01],\n       [ 8.47932141e-01,  3.88163622e-01,  6.57149313e-01],\n       [ 6.01510650e-01,  4.01372740e-01, -1.15815402e-04],\n       [ 5.10089822e-02, -7.83330700e-02,  7.48325452e-01]])\n```\n:::\n:::\n\n\nDönüştürücümüz `check_estimator` testlerinden de başarıyla geçiyor.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\ncheck_estimator(MinmaxScaler())\n```\n:::\n\n\n# Pipeline kullanımı\n\nTahminleyicilerimiz uygun şekilde hazırlandıysa artık bunları model seçimi, parametre arama (grid search), çapraz doğrulama (cross-validation), veri akışı (pipeline) işlemleri için kullanabilirsiniz.\n\nBir tahminleyiciyi bir veri akışı içinde kullanırken dikkat etmeniz gereken şeyler var:\n\n* Bir `Pipeline` nesnesinin `fit()` metodu, içindeki her tahminleyicinin `fit()` metodunu sırayla çalıştırır. Hattın bir ucundan giren veriyi dönüştürüp bir sonraki tahminleyiciye aktarır.\n* Hattın en sonundaki hariç, tahminleyicilerin hepsinin bir `fit()` veya `fit_transform()` metodu bulunmalıdır. Eğitim kümesinden farklı bir veri alacaklarsa, `transform()` metodları olmalıdır.\n* `Pipeline` nesnesi, hattın son adımındaki tahminleyiciyle aynı metodlara sahiptir. Son tahminleyici bir sınıflandırıcıysa, veri hattı da sınıflandırıcı olarak çalışır. Son aşamada bir dönüştürücü varsa, veri hattı da bir dönüştürücüdür.  \n\nVeri hatlarıyla ilgili daha fazla bilgi için [ilgili belgelere bakabilirsiniz](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}