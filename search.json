[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "The Notebook",
    "section": "",
    "text": "How to set up your data science projects\n\n\n\n\n\n\ndata science software\n\n\n\nTips from a minimalist perspective\n\n\n\n\n\nFeb 3, 2025\n\n\nKaan Öztürk\n\n\n\n\n\n\n\n\n\n\n\n\nMy lecture notes on R\n\n\n\n\n\n\nR\n\n\nlecture notes\n\n\nteaching\n\n\n\nand some rants about teaching the course\n\n\n\n\n\nDec 12, 2024\n\n\nKaan Öztürk\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the probability of rare events\n\n\n\n\n\n\noutliers\n\n\nextreme value theory\n\n\nprobability\n\n\n\n“It is most unlikely. But—here comes the big ‘but’—not impossible.” – Roald Dahl\n\n\n\n\n\nNov 29, 2024\n\n\nKaan Öztürk\n\n\n\n\n\n\n\n\n\n\n\n\nMichel’s method for numerical solution of equations of motion\n\n\n\n\n\n\nnumerical methods\n\n\ncomputational physics\n\n\n\nA simple but powerful method for solving equations of motion, developed by the late Prof. Curtis Michel.\n\n\n\n\n\nNov 22, 2024\n\n\nKaan Öztürk\n\n\n\n\n\n\n\n\n\n\n\n\nScikit-learn ile Kendi Tahminleyicilerimizi Yazmak\n\n\n\n\n\n\ndata science\n\n\n\nAlgoritmalarınızı scikit-learn yapısına uygun olarak yazın ve bu zengin kütüphanenin gücünü tam olarak kullanın\n\n\n\n\n\nNov 25, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\n\n\n\n\n\n\nSeçim Sandığında Bayes\n\n\n\n\n\n\nBayesian analysis\n\n\n\nArdışık oy uzunluğuyla sonuç tahmini\n\n\n\n\n\nJun 10, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "title": "Seçim Sandığında Bayes",
    "section": "",
    "text": "28 Mayıs’daki cumhurbaşkanlığı seçiminde oyların sayılmasını izlerken bir şey dikkatimi çekti: Bulunduğum sandıkta Kemal Kılıçdaroğlu’na (KK) arka arkaya çok sayıda oy çıkarken, Recep Tayyip Erdoğan’a (RTE) ardışık olarak ikiden fazla oy çıkmıyordu. Yani bir veya iki oy RTE, ardından KK oyları, ardından bir veya iki RTE oyu, vs.\nBunun üzerine aklıma şu soru geldi: Sandıkta bir adayın nihai oy oranını, sayım sırasında o aday için gördüğümüz en uzun ardışık oy uzunluğuna dayanarak (oyların anlık sayısını görmediğimizi varsayarak) tahmin edebilir miyiz?\nBu anlamlı bir soru, çünkü o sandıkta adayın (bilinmeyen) oy oranı yüksekse, en uzun zincirin uzunluğu da ona göre artacaktır. Sıfıra yakın bir olasılıksa uzunluk biri aşmayacak, yüzde yüze yakınsa da sayılan oy sayısına yakın olacak.\nBu azami zincir uzunluğu kaç oyun sayıldığına da bağlı. Çok çok sayıda oy varsa, büyük sayılar yasası gereği düşük oy oranında bile herhangi bir uzunlukta zincir görmek mümkün. Tersten bakarsak, azami zincir uzunluğunu ikide sabitlemekle, oy sayısı arttıkça oy oranı tahminimiz sıfıra yaklaşmak zorunda kalacak. Sandıklarda sadece birkaç yüz oy olduğu için bu sınırlara yaklaşmayacağız tabii. Yine de bu aşırı durumları akılda tutmak sonuçlarımızı kontrol etmek için yararlı.\nSandıkta adayın oy oranına \\(r\\) diyelim. Bunu bilmiyoruz, kestirmek istiyoruz. Sandıkta toplam 352 oy var. Bunların tamamı açıldıktan sonra tahmin edecek bir şey kalmıyor. Belli bir sayıda, mesela 100 oy açıldıktan sonra sonucu tahmin etmek istiyoruz.\nİlk aşamada, 100 oy içinde adayın arka arkaya aldığı oy zincirlerinin en uzununun 2 olması olasılığını bulalım.\nBu olasılık belki analitik yoldan bulunabilir ama ben yapamadım, o yüzden rastgele sayı üreten kısa bir program yazdım:\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import groupby\n\ndef oylar_üret(r, oy_sayısı):\n    return random.choices((\"RTE\",\"KK\"), weights=[r, 1-r], k=oy_sayısı)\n\ndef en_uzun_dizi_uzunluğu(x, değer=\"RTE\"):\n    return max(len(list(y)) for c,y in groupby(x) if c==değer)\n    \ndef olabilirlik(r, oy_sayısı, deneme=1000, gözlenen_uzunluk=2):\n    sayaç = 0\n    for i in range(deneme):\n        oylar = oylar_üret(r, oy_sayısı)\n        try:\n            if en_uzun_dizi_uzunluğu(oylar) == gözlenen_uzunluk:\n                sayaç += 1\n        except ValueError: # birinci seçenek hiç gözlenmediyse\n            continue\n    return sayaç / deneme\nBirinci fonksiyon oylar_üret(r, oy_sayısı) iki adaylı bir seçimde, birinci adayın oy oranı r olacak şekilde oy_sayısı kadar rastgele oy üretir. İkinci fonksiyon ise, birincinin çıktısını alıp onun içinde ardışık değer zincirinin en büyük uzunluğunu bulur.\nÖrnek:\nrandom.seed(20232023)\ns = oylar_üret(r=0.2, oy_sayısı=10)\ns\n\n['KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'RTE', 'RTE', 'KK']\nen_uzun_dizi_uzunluğu(s, \"RTE\")\n\n2\nÜçüncü fonksiyon olabilirlik, en uzun dizi uzunluğunun tam 2 olduğu durumların olasılığını, aynı oy oranı ve oy sayısıyla birçok rastgele sandık üreterek kestirir.\nÖrneğin, oy oranı 0.2 ise ve 10 adet oy açıldıysa, tam 2 uzunlukta en az bir dizi olması olasılığı yaklaşık 0.22 olur.\nolabilirlik(0.2, oy_sayısı=10, deneme=10000, gözlenen_uzunluk=2)\n\n0.2184\nTabii biz asıl oy oranını bilmiyoruz, o yüzden tersten gideceğiz: Farklı oy oranları için, 100 oy içinde tam 2 uzunlukta en az bir dizi görme olabilirliğini hesaplayacağız.\nCode\nr = np.linspace(0, 1, 101)\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nplt.plot(r, L)\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Olabilirlik\")\nplt.grid()\n\n\n\n\n\nOy oranına göre, iki uzunlukta en az bir dizi görme olasılığı\nBu sandıkta, RTE’nin oy oranının olasılık dağılımının 0.5’e doğru neredeyse sıfıra indiğini görüyoruz. Dağılımın tepesi 0.15 değerinde. Dağılım geniş bir aralığa yayılmış olduğu için sonuç hakkında net bir şey söylemek şimdilik zor olsa da, yarıyı geçme şansının çok düşük olduğu anlaşılıyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "title": "Seçim Sandığında Bayes",
    "section": "Bayes formülü",
    "text": "Bayes formülü\nBu noktada biraz duraklayıp bu problemi bir bayesçi kestirim alıştırması olarak ifade edelim. Oy oranı \\(r\\) ise ve \\(G\\) gözlemimizi (yani en uzun ardışık zincirin 2 uzunlukta olmasını) temsil ediyorsa, bu gözlem verilmiş olarak oy oranının olasılık dağılımını Bayes formülüyle ifade ederiz:\n\\[P(r | G) = \\frac{1}{P(G)}P(G|r)P(r)\\]\nÖnsel inanç: Burada \\(P(r)\\), bir şey gözlemeden önceki oy oranının dağılımıdır. Hiç bir şey gözlemediysek bunu nasıl bilebiliriz? Geçmiş tecrübelerimize dayanarak bir fikrimiz olabilir, veya her şey olabilir diyerek 0-1 arasında düzgün bir dağılım varsayabiliriz. Bu dağılıma önsel inanç (“prior belief”) denir.\nOlabilirlik: \\(P(G|r)\\) ifadesi, belli bir oy oranı \\(r\\) ile, gözlemimizin olasılığını verir. Yukarıdaki olabilirlik fonksiyonu tam bunu yapar. Bu faktörün teknik adı da zaten olabilirlik (“likelihood”).\nOlabilirlik fonksiyonu ile sonuçların nasıl üretildiğine dair modelimizi analizin içine katarız. Mesela burada, iki seçenekten birinin rastgele seçildiği bir Bernoulli modeli kullandık. Başka problemlerde başka modeller kullanılması gerekecektir.\nOlabilirlik, aslında aradığımız şeyin tersidir: Parametrelerin bilinen değeriyle çıktılar üretir. Oysa biz bu çıktılardan yola çıkarak parametrenin ne olduğuna dair bir fikir edinmek istiyoruz. Yukarıdaki Bayes formülü bu ters problemi çözmemizi sağlar.\nNormalleştirme: \\(P(G)\\) ifadesi bir normalleştirme sabitidir ve şu şekilde hesaplanabilir: \\[P(G) = \\sum_{r} P(G|r)P(r)\\] Ama genellikle bu sabiti doğrudan kullanmaya gerek olmaz.\nSonsal inanç: Yukarıdaki faktörleri birleştirerek elde ettiğimiz \\(P(r|G)\\) olasılığına sonsal inanç (“posterior belief”) denir. Bu, önsel inancımızın eldeki veriyle güncellenerek düzeltilmiş halidir."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "title": "Seçim Sandığında Bayes",
    "section": "Düzgün önsel ile tahminler",
    "text": "Düzgün önsel ile tahminler\nŞimdi bunu problemimize uygulayalım. Düzgün dağılmış bir önsel alalım, yani RTE’nin bu sandıktaki oy oranının 0 ile 1 arasında eşit olasılıkla herhangi bir değerde olabileceğini düşünelim.\n\n\nCode\nr = np.linspace(0, 1, 101)\nprior = np.ones_like(r) \nprior /= sum(prior) # normalizasyon\n\n# her oy oranı için olabilirliği hesapla\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.grid()\n\n\n\n\n\nÖnsel (prior) dağılım ve 100 oyda an az bir tane 2 uzunlukta zincir olabilirliği ile sonsal (posterior) dağılım\n\n\n\n\nÖnceki grafiğin aynısı çıktı, ki sabit bir önsel aldığımız için böyle olması gerek (olabilirliği sabit bir sayıyla çarpıyoruz)\nBurada tek bir oy oranı tahmini çıkarmıyoruz. Bayesçi kestirim bize tek tahminler değil, tahmin edilecek değişken için bir olasılık dağılımı verir. İsterseniz bu dağılımdan tek tahminler (point estimate) çıkarabilirsiniz. Örneğin en yüksek olasılıklı değeri 0.15, ortalama değeri 0.17 olarak bulabiliriz.\nBaşka bir özet sayı, sonsal dağılımın ortasında %90 alanı kaplayan aralığın (“highest posterior density interval”) sınırlarıdır. Buna göre RTE’nin oy oranını, %90 olasılıkla (0.06, 0.29) arasında tahmin edebiliriz.\n\n\nCode\nprint(f\"En muhtemel değer: { r[np.argmax(posterior)] : .2f}\")\nprint(f\"Ortalama değer: { np.sum(r*posterior): .2f}\")\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\nEn muhtemel değer:  0.14\nOrtalama değer:  0.17\n%90 yoğunluk aralığı: 0.06  - 0.29"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "title": "Seçim Sandığında Bayes",
    "section": "Güncelleme",
    "text": "Güncelleme\nDiyelim 100 oy daha sayıldı ve RTE’ye yine en fazla 2 uzunlukta ardışık zincirler gözlediniz. Bu yeni veriyle RTE’nin oy oranına dair inancınızı (sonsal dağılımı) güncelleyebilirsiniz.\nBayesci analizin güzel tarafı, bunu yaparken sıfırdan başlamak zorunda olmamanız. Bir önceki adımda bulduğunuz sonsal dağılımı bu sefer önsel dağılım olarak kullanıp aynı işlemi tekrarlayabilirsiniz.\nOlabilirlik hesabını tekrarlamak zorunda değilsiniz, çünkü parametreler tamamen aynı olduğu için fonksiyon da aynı olacak.\n\n\nCode\nprior = posterior\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 200 oy\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.08  - 0.24\n\n\n\n\n\nYeni veri ile tahmin güncelleme\n\n\n\n\nBu yeni veri ile sonsal dağılım biraz daha daraldı. Güncellemeden sonra %90 yoğunluk aralığı 0.08-0.24 oldu."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı olabilirlikler",
    "text": "Farklı olabilirlikler\nSandıktaki oyların sayımı bittiğinde RTE’nin 352 oydan 83’ünü aldığı görüldü. Yani gerçekleşen oran 0.24 olmuş. Bu değer yukarıda bulduğumuz %90 aralığının tam sınırında. Sonsal dağılımımız olması gerekenin biraz altında kalmış görünüyor.\nBunun birkaç sebebi olabilir. Birincisi, ardışık oy dizisi uzunluğu iyi bir gösterge olmayabilir. İkincisi, benim gözlemim yanlış olabilir. Belki başlarda arka arkaya üç tane RTE oyu çıkmıştır da, ben o sırada ardışıklığa dikkat etmediğim için kaçırmış olabilirim.\nHesabı bu ihtimale göre iki aşamada tekrarlayalım. İlk 100 oy içinde en uzun RTE zinciri 3 uzunlukta olsun, ondan sonraki 100 oy içinde 2 uzunlukta olsun.\n\n\nCode\nprior1 = np.ones_like(r)\nprior1 /= sum(prior1)\n\nL1 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=3) for rr in r]\nposterior1 = L1*prior1\nposterior1 /= sum(posterior1) # normalizasyon\n\nprior2 = posterior1\nL2 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior2 = L2*prior2\nposterior2 /= sum(posterior2) # normalizasyon\n\nplt.plot(r, prior1, label=\"prior 1\")\nplt.plot(r, posterior1, label=\"posterior 1 (prior 2)\")\nplt.plot(r, posterior2, label=\"posterior 2\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior2))][0]}  - {r[(np.cumsum(posterior2)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.11  - 0.3\n\n\n\n\n\nİlk 100 oy içinde 3 uzunlukta en az bir zincir, ikinci 100 oy içinde 2 uzunlukta en az bir zincir gözlendiğinde sonsal dağılımlar\n\n\n\n\nİlk 100 oyda üçlü diziler olduğu için, birinci sonsal dağılımımız sağa kaymış, yani yüksek oy oranlarının olasılığı artmış. Bunun sonucu olarak da 200 oy gözlendikten sonraki sonsal, bir önceki çözümümüze göre daha sağa kaymış. Böylece %90 yoğunluk aralığı daha geniş ve gerçekleşen oranı içeriyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı önsel: Geçmiş seçimden bilgi aktarma",
    "text": "Farklı önsel: Geçmiş seçimden bilgi aktarma\nSon olarak, farklı bir önsel kullanmayı deneyelim.\nŞimdiye kadar kullandığımız önsel, bir aday için bütün oy oranlarının eşit olasılıkta olduğunu varsayıyordu. Oysa bunun doğru olmadığını geçmiş tecrübemizden biliyoruz. Seçimin birinci turunda aynı sandıkta 357 oyun 81’i RTE’ye gittiğini gözlemiştik. Bu bilgiyle, Beta dağılımına uyan bir önsel belirleyebiliriz:\n\\[P(r) = C\\ r^{81} (1-r)^{276}\\]\nYine, ilk 100 oyda en uzun zincirin 2 uzunlukta olduğunu varsayarak sonsal hesaplayalım.\n\n\nCode\nprior = r**81 * (1-r)**276\nprior /= sum(prior)\n\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior = L*prior\nposterior /= sum(posterior)\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.xlim((0.1,0.4))\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.19  - 0.25\n\n\n\n\n\nGeçmiş seçimin oy oranlarıyla oluşturulan önsel ve bununla elde edilen sonsal dağılım.\n\n\n\n\n100 oy sayımında en fazla 2 uzunlukta zincir görmekle, sonsal olasılık dağılımının hafifçe sola kaydığını, yani oy oranının daha düşük ihtimallere kaydığını görüyoruz.\nAyrıca, daha belirli (informative) bir önsel seçmekle, sonsal dağılımımızın %90 yoğunluk aralığı daraldı, yani belirsizliği daha az bir sonuç elde ettik."
  },
  {
    "objectID": "posts/en/2024/Estimating-probability-of-rare-events/index.html",
    "href": "posts/en/2024/Estimating-probability-of-rare-events/index.html",
    "title": "Estimating the probability of rare events",
    "section": "",
    "text": "Suppose you have a set of data, which is concentrated around some mode value, as usual. The farther out you go, the fewer points you have. In other words, large deviations from the mode is unlikely. The larger, the unlikelier. (A very small value may count as a large deviation in the other direction, depending on the distribution.)\nYou might want to have a numerical estimate of how unlikely these outliers are. That is, what is the probability to see such a big deviation, or bigger?1\nThis probability would be useful for interpreting your results, or for detecting an anomaly. If you find a once-a-year data point occurring two or three times within a week, you might want to investigate.\nHowever, estimating the probability of rare events is tricky. True, if you know the parent distribution of the data, you can easily use the cumulative distribution function2 to calculate that probability. But, with real data, we don’t know the parent distribution with certainty.\nWithout a model of the parent distribution, we can use the empirical distribution function. This is simple enough: Just count the fraction of your data points below a value. However, the empirical distribution function will not help with a new large value. The survival function for this new value will just be zero, simply because the data’s larger than any we have seen so far.\nWe can use Extreme Value Theory to estimate the tail probabilities based on empirical data only. Specifically, the theory states that data points larger than a threshold are distributed, approximately, as the Generalized Pareto distribution.\nLet’s see how this theory is applied."
  },
  {
    "objectID": "posts/en/2024/Estimating-probability-of-rare-events/index.html#the-peak-over-threshold-approach",
    "href": "posts/en/2024/Estimating-probability-of-rare-events/index.html#the-peak-over-threshold-approach",
    "title": "Estimating the probability of rare events",
    "section": "The Peak-Over-Threshold approach",
    "text": "The Peak-Over-Threshold approach\nSuppose that we have a set of observations \\(\\lbrace x_1, x_2, \\ldots, x_n\\rbrace\\). The values \\(x_i\\) are realizations of a random variable \\(X\\), which is distributed according to an unknown cumulative distribution function \\(F_X(x) = \\mathrm{Pr}( X&lt;x)\\). In practice we do not know the parent distribution \\(F_X(x)\\).\n\n\n\n\n\n\n\n\n\nIf we are mainly interested in the behaviour of rare values, the Peaks-Over-Threshold method provides a good fit to the distribution’s tail. The idea is to select data points above a given threshold \\(u\\) and build a conditional probability distribution with them.\nWe first define the variable \\(Y\\equiv X-u\\), called the excess, which gives how much \\(X\\) exceeds \\(u\\).\nAlso define the conditional excess distribution function \\(F_{Y|u}(y)\\) as:\n\\[\\begin{equation}\n    F_{Y|u}(y) = \\mathrm{Pr}(X-u \\leq y | X&gt;u).\n\\end{equation}\\]\nThe condition is necessary because we assume that the variable \\(X\\) is larger than the threshold \\(u\\).\nUsing the definition of conditional probability, this can be rewritten as\n\\[\\begin{align}\n    F_{Y|u}(y) &=& \\mathrm{Pr}(X \\leq y+u | X&gt;u) \\\\\n    &=& \\frac{\\mathrm{Pr}(X \\leq y+u , X&gt;u)}{\\mathrm{Pr}(X&gt;u)}\\\\\n    &=& \\frac{F_X(u+y) - F_X(u)}{1-F_X(u)}\\\\\n    &=& \\frac{F_X(x) - F_X(u)}{1-F_X(u)}\n\\end{align}\\]\nSolving for \\(F_X(x)\\), we get:\n\\[\\begin{equation}\n    F_X(x) = \\left(1-F_X(u)\\right)F_{Y|u}(y) + F_X(u)\n\\end{equation}\\]\nWe estimate \\(F_X(u)\\) using the data directly, as the fraction of all observations less than or equal to \\(u\\):\n\\[\\begin{equation}\n\\hat{F}_X(u) = \\frac{n-N_u}{n}\n\\end{equation}\\] where \\(n\\) is the number of all observations and \\(N_u\\) is the number of observations equal or larger than \\(u\\).\nThe remaining piece is \\(F_{Y|u}(y)\\). To estimate it, we use the Pickands-Balkema-de Haan theorem from extreme value theory. The theorem states that, for large \\(u\\), \\(F_{Y|u}\\) is well-approximated by the Generalized Pareto Distribution \\(G_{k,\\sigma}(y)\\), where \\[\\begin{equation}\n    \\lim_{u\\rightarrow\\infty} F_{Y|u}(y) = G_{k,\\sigma}(y) = \\begin{cases}\n    1 - \\left(1 + \\frac{ky}{\\sigma}\\right)^{-1/k} & \\text{if } k\\neq 0 \\\\\n    1 - \\mathrm{e}^{-y/\\sigma} & \\text{if } k=0 \\\\\n    \\end{cases}\n\\end{equation}\\] Here \\(\\sigma &gt; 0\\) is the scaling parameter and \\(k\\) is the shape parameter, also known as the tail index.\nPlugging in the Pickands-Balkema-de Haan approximation, we get an estimation for the distribution \\(F_X(x)\\) for large \\(u\\) and \\(x&gt;u\\): \\[\\begin{equation}\n    \\hat{F}_X(x) = \\left(1 - \\frac{n-N_u}{n}\\right)G_{k,\\sigma}(x-u) + \\frac{n-N_u}{n}\n\\end{equation}\\] Finally, after simplification, we get the tail approximation for the unknown distribution \\(F_X(x)\\): \\[\\begin{equation}\n    \\hat{F}_X(x) = 1 + \\frac{N_u}{n}\\left(G_{k,\\sigma}(x-u)-1\\right).\n\\end{equation}\\] The parameters \\(N_u\\), \\(k\\) and \\(\\sigma\\) should be estimated from given data \\(\\lbrace x_1, x_2, \\ldots, x_n\\rbrace\\) and fixed \\(u\\).\nThe approximation is gets better as the threshold \\(u\\) increases. However, if we set the threshold too large, there will be an insufficient amount of data to fit the distribution parameters. To balance these requirements, we set \\(u\\) such that 10% of data is above it."
  },
  {
    "objectID": "posts/en/2024/Estimating-probability-of-rare-events/index.html#implementation",
    "href": "posts/en/2024/Estimating-probability-of-rare-events/index.html#implementation",
    "title": "Estimating the probability of rare events",
    "section": "Implementation",
    "text": "Implementation\nHere we write a Python implementation of the this cumulative distribution function. Up to the threshold \\(u\\), the function is equal to the empirical distribution function. Above the threshold, it returns the tail approximation.\nWe do not use the threshold \\(u\\) directly, but we set a fixed percentile \\(q\\) (e.g. \\(q=0.9\\)). The two are related with the empirical CDF as \\(q = F_n(u)\\). Using the percentile makes the implementation easier: Tail points are simply selected as the \\(q\\)-quantile of the data.\nTo estimate the parameters of the Generalized Pareto Distribution, we use the method implemented in Scipy. Internally, SciPy uses maximum-likelihood estimation to infer the parameters.\n\nimport numpy as np\nfrom scipy import stats\n\ndef ecdf_tail_approx(x, q=0.9):\n    \"\"\"Return the empirical cumulative distribution function (CDF)\n    where the tail has the Generalized Pareto form.\n    \n    Parameters\n    ----------\n    x : numpy array\n        The input data.\n    q : float, optional. Default: 0.9\n        The quantile threshold for switching to the conditional \n        excess distribution function. Must be between 0 and 1, close to 1.\n        The excess threshold u satisfies q = F(u).\n        \n    Returns\n    -------\n    Cumulative distribution function\n    \n    \"\"\"\n    x = np.sort(x)\n    def raw_ecdf(v):\n        return np.searchsorted(x, v, side='right') / x.size\n    # Select the points above the threshold\n    tailx = x[raw_ecdf(x)&gt;=q]\n    # Determine the parameters of the Generalized Pareto Distribution\n    shape, loc, scale = stats.genpareto.fit(tailx)\n    # The CDF to be returned:\n    def result(v):\n        # The raw empirical CDF value based on data:\n        r = raw_ecdf(v)\n        # The conditional excess CDF at the tail\n        t = 1 + tailx.size/x.size * (stats.genpareto.cdf(v, shape, loc=loc, scale=scale) - 1)\n        # Replace the tail with the excess CDF\n        return np.select([r&lt;q, r&gt;=q], [r, t])\n        \n    return result\n\nLet’s try this approach with some common probability distributions: Generate a set of random values from a known distribution and check how well the tail approximation is."
  },
  {
    "objectID": "posts/en/2024/Estimating-probability-of-rare-events/index.html#comparison-with-known-distributions",
    "href": "posts/en/2024/Estimating-probability-of-rare-events/index.html#comparison-with-known-distributions",
    "title": "Estimating the probability of rare events",
    "section": "Comparison with known distributions",
    "text": "Comparison with known distributions\nIn all the following, we generate a sample from a known distribution and fit an extreme-value distribution to the tail of that sample. We repeat this 50 times and generate one curve for each sample, in order to show the variability in the result.\nWe set the threshold quantile to \\(q=0.9\\), which means roughly one-tenth of the sample points are used to fit the tail distribution.\n\n\nShow the code\nimport matplotlib.pyplot as plt\n\ndef plot_dist(dist, n, title, q=0.9, repeat=1):\n    \"\"\"_summary_\n\n    Parameters\n    ----------\n    dist : scipy.stats._continuous_distns.rv_continuous instance\n        A continuous distribution object such as norm, expon, poisson, beta, etc.\n    n : int\n        sample size\n    title : str\n        plot title\n    q : float, optional. Default 0.9\n        The threshold quantile, specifying where the tail approximation begins.\n    repeat : int, optional. Default 1\n        Number of times to repeat sampling, fitting, and plotting.\n    \"\"\"\n    u = dist.ppf(q)\n    x = np.linspace(1,10,100)\n    \n    y = dist.rvs(size=n)\n    plt.plot(x, ecdf_tail_approx(y, q=q)(x), \"C1\", alpha=0.5, label=\"Tail-approximated CDF\")\n    for r in range(1,repeat):\n        y = dist.rvs(size=n)\n        plt.plot(x, ecdf_tail_approx(y, q=q)(x), \"C1\", alpha=0.5)\n\n    plt.plot(x, dist.cdf(x), label=\"True CDF\")\n\n    plt.xlim(u,10)\n    plt.ylim([q,1])\n    plt.grid()\n    plt.title(title)\n    plt.xlabel(\"x\")\n    plt.ylabel(\"Cumulative distribution function $F(x)$\")\n    plt.legend()\n    oldticks = plt.xticks()\n    # add threshold u to ticks\n    plt.xticks(oldticks[0].tolist()+[u], oldticks[1]+[plt.Text(u,0,\"u\")])\n\n\n\nNormal distribution\n\n\nShow the code\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_dist(stats.norm(scale = 3), 1000, \"Normal, st.dev=2, 1000 samples\", q=0.90, repeat=50)\nplt.subplot(1,2,2)\nplot_dist(stats.norm(scale = 3), 10000, \"Normal, st.dev=2, 10000 samples\", q=0.90, repeat=50)\n\n\n\n\n\n\n\n\n\nCurves based on 1000 sample points3 (left panel) show a lot of variation. The normal distribution tail falls off rapidly as \\(\\mathrm{e}^{-x^2}\\), so there are very few points on the far tail, which makes the fit very variable. Curves based on samples with size 10000 (right panel) are more aligned with the true CDF.\n3 remember that only about 100 of them are above the threshold\n\nExponential distribution\nThe exponential distribution’s tail does not fall as quickly as the normal, so most of the approximated curves are around the parent CDF curve.\n\n\nShow the code\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_dist(stats.expon(scale = 2), 1000, \"Exponential, scale=2, 1000 samples\", q=0.9, repeat=50)\nplt.subplot(1,2,2)\nplot_dist(stats.expon(scale = 2), 10000, \"Exponential, scale=2, 10000 samples\", q=0.90, repeat=50)\n\n\n\n\n\n\n\n\n\n\n\nPareto distribution\nFinally, the Pareto distribution has a fat tail, falling off as a negative power: \\(F(x; b) = 1 - \\frac{1}{x^b}\\)\nThis is also known as a power law.\nBecause of this fat tail, there are a significant number of points in higher values. As a result, tail approximations have smaller variation.\n\n\nShow the code\nplt.figure(figsize=(12,5))\nplt.subplot(1,2,1)\nplot_dist(stats.pareto(b = 2.5), 1000, \"Pareto, b=2.5, 1000 samples\", q=0.9, repeat=50)\nplt.subplot(1,2,2)\nplot_dist(stats.pareto(b = 2.5), 10000, \"Pareto, b=2.5, 10000 samples\", q=0.9, repeat=50)"
  },
  {
    "objectID": "posts/en/2024/Estimating-probability-of-rare-events/index.html#what-about-multivariate-distributions",
    "href": "posts/en/2024/Estimating-probability-of-rare-events/index.html#what-about-multivariate-distributions",
    "title": "Estimating the probability of rare events",
    "section": "What about multivariate distributions?",
    "text": "What about multivariate distributions?\nThe classic discussion above applies to a single random variable. How about the jont extreme-value distribution of several variables, \\(F(x_1, x_2,\\cdots,x_n)\\)?\nMultivariate theory of extreme values is more complicated, involving fitting the dependency structure and combining them with copulas. For an example, see Dutfoy et al. (2014)4\n4 Dutfoy, Anne, Parey, Sylvie and Roche, Nicolas. “Multivariate Extreme Value Theory - A Tutorial with Applications to Hydrology and Meteorology” Dependence Modeling, vol. 2, no. 1, 2014, pp. 000010247820140003. https://doi.org/10.2478/demo-2014-0003As a pragmatic alternative, we can first convert the multidimensional data points into scalar anomaly scores. These can be obtained by using anomaly detection methods such as Isolation Forest and Local Outlier Factor, or by evaluating the Mahalanobis distance of data points. Then, we can construct a tail distribution based on these scores. However, this is out of the scope of this post."
  },
  {
    "objectID": "posts/en/2024/r-lecture-notes/index.html",
    "href": "posts/en/2024/r-lecture-notes/index.html",
    "title": "My lecture notes on R",
    "section": "",
    "text": "Between 2017 and 2022, I was teaching two mass courses on introductory programming at Boğaziçi University. One of them was CMPE140, R programming for economics and management students. I converted my lecture notes for this course to a book-like HTML format using Quarto. They are now available on my website.\nWhen I took it up, I did not know R, even though I’ve been teaching C and Python for many years. Of course, the basics of all structured programming languages are alike, so you can pick them up pretty easily: Variable declarations, decisions, loops, subroutines take you most of the way. For a course designed to be an introduction to programming, these are the main teaching objectives anyway.\nThe course teaches only base R, without using any libraries. This is by design. Modern R users use advanced data science modules such as dplyr, ggplot, and other tidyverse libraries. But the course’s scope was limited to teaching algorithmic thinking and basic programming, so these tools were left out by choice. However, I added some R-specific topics such as data visualization, even linear regression. I wanted the students to use their R skills in later courses, even improve them.\nRoughly half of the notes are on general programming concepts, presented in R, and the rest on R-specific topics. You might find the order of lectures a bit odd (e.g., plotting before functions, functions before if statements). Sometimes we shifted some topics up, so that we could design programming questions about them in exams and assignments.\nBefore the course was passed to me, grading was based on multiple-choice exams. However, I believed that programming skills should be tested by having students write actual programs. Luckily, our department had a very secure lab system, which was used extensively for both teaching and testing. I adopted that system for CMPE140, too.\nOur quizzes and other exams were actual programming problems. Students were free to use RStudio to write and execute their programs, and check the results against test cases we provided. Once they were confident, they would submit their solutions. There was no internet access, but they could access the inline help system.\nThe upside of this method is that students could use trial and error to get to the right result, guided by the computer. The downside is that the time constraint would cause stress for some students. If I were to give the course again, I would diversify grading by including some multiple-choice quizzes.\nGrading was automated, using scripts that run the code and compare the output with the expected answer. So we have a binary score for every sub-problem. This method unfortunately prevents partial credit for “going the right path” but not getting the correct result. But with hundreds of students, manual grading was infeasible. We tried to subdivide tasks as much as possible, in order to give as much credit as possible.\nCovid-19 forced us to change our ways. I could not possibly administer exams in a cramped computer lab. Grades became based on take-home assignments only. These were more challenging than exam problems, but I let them work for a full weekend to work on them.\nTo check against cheating, we used the Moss system. I was pleased to see that copying, though present, was not rampant. We took action for those we detected.\nMy teaching ended in Spring of 2022, when the appointed rector started to wreak havoc at Boğaziçi University. By that time, I had switched to part-time teaching. He did not renew my contract, against the wishes of the department, mere days before the start of the semester. Another lecturer had to take over the class with a very short preparation period.\nIf I continued, I certainly would have to change the delivery of the course, due to the wide availablity of LLMs. I gave some of my assignments to ChatGPT, and its answer was almost exactly the same as mine. Obviously I could not rely on take-home assignments, even with similarity checks. I would have returned to using in-class programming exams. I would also have to change the delivery of the course, possibly integrating it with LLMs, discussing things like the strong and weak points of LLMs, or how to check the correctness of the LLM’s answer.\nSome words on my tech stack: I had prepared the notes as Jupyter notebook, which allowed me to integrate verbal descriptions with actual R code cells. During class, I would run the cells one by one, discuss the outcome, change the cell content to illustrate some point, run again, discuss again, etc.\nWith the RISE extension, I could present the notebook as a slide deck, displaying one or two cells at a time. It was not a static slideshow; I could run the cells in real time.\nI uploaded my set of lecture notebooks to a GitHub repository, which also housed lab and problem-solving documents. We configured the repo to use the Binder service. With Binder, students could run the notebooks in the could, without having to donwload them or setting up a Jupyter environment (we were endorsing RStudio instead).\nHaving all the notes in Jupyter notebook form had another benefit: The Quarto publishing system can directly work with them. I had already prepared this blog with Quarto, without having any HTML or web design knowledge.\nI converted all ipynb files to qmd (Quarto markdown) files with quarto convert, because I had some trouble re-establishing an R kernel with Jupyter notebook in the same virtual environment. Quarto render engine would then run R code blocks automatically, embedding the output in the document.\nI did some editing on the notes, but it was either some minimal formatting for Quarto, or adding more explanations about the lecture (I do the explanations verbally in class, so written notes had some gaps).\n\n\n\nCitationBibTeX citation:@online{m._kaan2024,\n  author = {M. Kaan , Öztürk},\n  title = {My Lecture Notes on {R}},\n  date = {2024-12-12},\n  url = {https://mkozturk.com/posts/en/2024/r-lecture-notes/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nM. Kaan, Öztürk. 2024. “My Lecture Notes on R.” December\n12, 2024. https://mkozturk.com/posts/en/2024/r-lecture-notes/."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "",
    "text": "Prof. F. Curtis Michel (1934-2015) was the Andrew Hays Buchanan Professor of Astrophysics at Rice University. At a departmental seminar on March 3, 2005, he described a method for numerically solving differential equations of motion. He developed the method independently during his work on stellar magnetospheres, but he did not claim any originality. As far as I know, Prof. Michel did not publish this method. I recently found my notes from that talk and I decided to write it up here."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#the-algorithm",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#the-algorithm",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "The algorithm",
    "text": "The algorithm\nLet \\(\\mathbf{x}\\in \\mathbb{R}^3\\) be the position and \\(\\mathbf{v}\\in \\mathbb{R}^3\\) be the velocity of a particle at a given time. The time evolution of these variables obey the following equation of motion:\n\\[\n\\begin{eqnarray}\n\\dot{\\mathbf{x}} &=& \\mathbf{v}\\\\\n\\dot{\\mathbf{v}} &=& \\mathbf{a}(\\mathbf{x},\\mathbf{v})\n\\end{eqnarray}\n\\]\nwhere the acceleration \\(\\mathbf{a}(\\mathbf{x},\\mathbf{v})\\) is a given function of position and velocity.\nSuppose at time step \\(t_n\\) the position \\(\\mathbf{x}_n\\) and velocity \\(\\mathbf{v}_n\\) are given, and we want to estimate the \\(\\mathbf{x}_{n+1}\\) and velocity \\(\\mathbf{v}_{n+1}\\) at the next time step \\(t_{n+1}\\equiv t_n+\\tau\\). The scheme proposed by Michel is as follows:\nDefine an intermediate step \\(\\mathbf{x}_{n+\\textonehalf}\\):\n\\[\n\\mathbf{x}_{n+\\textonehalf} \\equiv \\mathbf{x}_{n} + \\frac{\\tau}{2}\\mathbf{v}_{n}\n\\]\nThen, the velocity is updated as:\n\\[\n\\begin{eqnarray}\n\\mathbf{v}_{n+1} = \\mathbf{v}_{n} + \\tau \\mathbf{a}\\left(\\mathbf{x}_{n+\\textonehalf}, \\frac{1}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n})\\right)\n\\end{eqnarray}\n\\]\nand the position is updated as:\n\\[\n\\begin{eqnarray}\n\\mathbf{x}_{n+1} &=& \\mathbf{x}_{n+\\textonehalf} + \\frac{\\tau}{2}\\mathbf{v}_{n+1} \\\\\n&=& \\mathbf{x}_{n} + \\frac{\\tau}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n})\n\\end{eqnarray}\n\\]\nHere is a representation of the scheme: \nThe update equations have \\(\\mathbf{v}_{n+1}\\) on the right-hand side, making this an implicit scheme. Implicit schemes have the additional burden of solving the equations for the values at step \\(n+1\\); however, they are generally more accurate and stable."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-motion-under-magnetic-field",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-motion-under-magnetic-field",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Application: Motion under magnetic field",
    "text": "Application: Motion under magnetic field\nA particular form of velocity-dependent acceleration is provided by the magnetic field:\n\\[\n\\mathbf{a}(\\mathbf{x},\\mathbf{v}) = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}(\\mathbf{x})\n\\]\nwhere \\(q\\) and \\(m\\) are the electric charge and mass of the accelerated particle, respectively. Because of the cross product, the acceleration is always perpendicular to the velocity. Therefore, the speed and the kinetic energy of the particle is constant in time.\nThe velocity update equation becomes:\n\\[\n\\mathbf{v}_{n+1} = \\mathbf{v}_{n} + \\frac{\\tau q}{2m}\\left(\\mathbf{v}_{n+1}+\\mathbf{v}_{n}\\right)\\times \\mathbf{B}\\left(\\mathbf{x}_n + \\frac{\\tau}{2}\\mathbf{v}_n\\right)\n\\]\nIn order to solve for \\(\\mathbf{v}_{n+1}\\), we write the cross product as a matrix-vector product: Let \\(\\mathbf{v} = [v_x, v_y, v_z]^\\intercal\\) and \\(\\mathbf{B} = [B_x, B_y, B_z]^\\intercal\\). Then\n\\[\n\\mathbf{v}\\times\\mathbf{B} = \\begin{bmatrix}\n0 & B_z & -B_y \\\\\n-B_z & 0 & B_x \\\\\nB_y & -B_x & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nv_x \\\\\nv_y \\\\\nv_z\n\\end{bmatrix} := \\mathbb{B}\\mathbf{v}\n\\]\nUsing the matrix \\(\\mathbb{B}_n\\) constructed from \\(\\mathbf{B}\\left(\\mathbf{x}_n + \\frac{\\tau}{2}\\mathbf{v}_n\\right)\\), the velocity update equation can be written as:\n\\[\n\\mathbf{v}_{n+1} = \\left(\\mathbb{I}+\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n + \\frac{\\tau q}{2m}\\mathbb{B}_n\\mathbf{v}_{n+1}\n\\] where \\(\\mathbb{I}\\) is the 3-by-3 identity matrix. Now we can collect \\(\\mathbf{v}_{n+1}\\) terms on the left: \\[\n\\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_{n+1} = \\left(\\mathbb{I}+\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n\n\\]\nIt is straightforward to show that the determinant \\[\\det \\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right) = \\left(\\frac{q\\tau}{2m}\\right)^2\\left(B_x^2+B_y^2+B_z^2\\right)\\] is nonzero as long as there is a magnetic field present. Then, the matrix is invertible and the next step velocity can be solved as:\n\\[ \\mathbf{v}_{n+1} = \\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)^{-1}\\left(\\mathbb{I}+ \\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n\\]\nThe position vector is then updated using the current and next velocity vectors:\n\\[ \\mathbf{x}_{n+1} = \\mathbf{x}_{n} + \\frac{\\tau}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n}) \\]"
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#kinetic-energy-preservation",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#kinetic-energy-preservation",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Kinetic energy preservation",
    "text": "Kinetic energy preservation\nPhysically, a magnetic field does not change the kinetic energy of the particle. Its acceleration serves only to deflect the trajectory sideways. To see this, consider the work done on the particle by the magnetic field force\n\\[\\begin{eqnarray}\nW &=& \\int \\mathbf{F}(\\mathbf{x})\\mathrm{d}\\mathbf{x}\\\\\n&=& \\int \\mathbf{F}(\\mathbf{x})\\cdot\\mathbf{v}\\mathrm{d}t\\\\\n&=& \\int (q\\mathbf{v}\\times \\mathbf{B})\\cdot\\mathbf{v}\\mathrm{d}t\n\\end{eqnarray}\n\\] The integrand is identically zero, because the force is prependicular to the velocity at all times. There is no work done on the particle, so its kinetic energy stays constant.\nMichel’s method has the merit of perfectly preserving the kinetic energy under a magnetic field. Consider the velocity update formula:\n\\[\n\\mathbf{v}_{n+1} = \\mathbb{M}\\mathbf{v}_n\n\\] where \\[\n\\mathbb{M}:=\\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)^{-1}\\left(\\mathbb{I}+ \\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\n\\]\nBy explicit calculation we can show that the matrix \\(\\mathbb{M}\\) is orthogonal, that is: \\[\n\\mathbb{M}\\mathbb{M}^\\intercal=\\mathbb{I},\n\\]\nOrthogonal matrices transform vectors without changing their length. To see this, multiply both sides of the velocity update formula with \\(\\mathbf{v}_{n+1}^\\intercal\\).\n\\[\n\\begin{eqnarray}\n\\mathbf{v}_{n+1}^\\intercal\\mathbf{v}_{n+1} &=& \\mathbf{v}_{n+1}^\\intercal\\mathbb{M}\\mathbf{v}_n\\\\\n\\mathbf{v}_{n+1}^\\intercal\\mathbf{v}_{n+1} &=& \\mathbf{v}_n^\\intercal\\mathbb{M}^\\intercal\\mathbb{M}\\mathbf{v}_n\\\\\n|\\mathbf{v}_{n+1}|^2 &=& \\mathbf{v}_n^\\intercal\\mathbb{I}\\mathbf{v}_n\\\\\n|\\mathbf{v}_{n+1}|^2 &=& |\\mathbf{v}_{n}|^2\\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#simulation-of-a-charged-particle-in-a-dipolar-magnetic-field",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#simulation-of-a-charged-particle-in-a-dipolar-magnetic-field",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Simulation of a charged particle in a dipolar magnetic field",
    "text": "Simulation of a charged particle in a dipolar magnetic field\nTo illustrate the method, we will trace the path of a charged test particle under the field of a magnetic dipole fixed at the origin. The dipole moment is set to be in the negative-\\(\\mathbf{z}\\) direction, imitating the Earth’s magnetic field.\nThen, the magnetic field vector in cartesian coordinates is:\n\\[\n\\mathbf{B}(\\mathbf{x}) = -\\frac{B_0}{r^5}\n\\begin{bmatrix}\n3xz \\\\\n3yz \\\\\n3z^2-r^2\n\\end{bmatrix}\n\\] where \\(\\mathbf{x}=[x,y,z]^\\intercal\\) and \\(r^2 = x^2+y^2+z^2\\). The coefficient \\(B_0\\) specifies the field strength at unit distance on the equatorial plane (\\(z=0\\) and \\(r=1\\)).\nLet’s write a program to implement the schema for this particular field:\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\nq = -1 # particle charge\nm = 1  # particle mass\nB0 = 100 # field strength at unit distance on the z=0 plane\n\ndef dipole_field(x):\n    # Field of a magnetic dipole located at origin, pointing to the negative-z direction\n    # North pole is up, south pole is down.\n    # x  : Three-element array of position\n    # B0 : field strength at unit distance from origin\n    rsq = (x**2).sum()\n    return -B0 / (rsq**(5/2)) * np.array([3*x[0]*x[2], 3*x[1]*x[2], (3*x[2]**2-rsq)])\n\ndef update(x, v, dt, field):\n    # the half-step\n    x1 = x + v*dt/2\n    # Magnetic field vector at the half-step\n    B = field(x1)\n    # Magnetic field matrix for cross-product\n    B_mat = np.array([\n        [0, B[2], -B[1]],\n        [-B[2], 0, B[0]],\n        [B[1], -B[0], 0]\n        ])\n    # the left matrix\n    M1 = np.identity(3) - dt*q/(2*m)*B_mat\n    # the right matrix\n    M2 = np.identity(3) + dt*q/(2*m)*B_mat\n    # Solve for the next step velocity\n    v_next = np.linalg.solve(M1, np.dot(M2, v))\n    # Determine the next step position\n    x_next = x + (v_next + v)*dt/2\n\n    return x_next, v_next\n\nNote that the function update() can be used with any magnetic field that is defined as a function, as shown above.\nWe initialize the particle at \\(\\mathbf{x} = [3, 0, 0]^\\intercal\\) with velocity \\(\\mathbf{v} = [0, 0.4, 0.5]^\\intercal\\), and follow it for 3000 steps, with time step 0.1\n\nx = np.array([3,0,0])\nv = np.array([0,0.4,0.5])\ndt = 0.1\n\nt_steps = np.arange(0,3000*dt,dt)\n# store intermediate steps:\nx_steps = [x]\nv_steps = [v]\nfor t in t_steps:\n    x, v = update(x,v,dt,dipole_field)\n    x_steps.append(x)\n    v_steps.append(v)\n\nThe resulting path of the particle can then be visualized:\n\nx_t = [_[0] for _ in x_steps]\ny_t = [_[1] for _ in x_steps]\nz_t = [_[2] for _ in x_steps]\nax = plt.figure().add_subplot(projection='3d')\nax.plot(x_t, y_t, z_t)\nax.scatter(x_t[0], y_t[0], z_t[0], c=\"C1\", s=10)\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\n\n\n\n\nThe orange dot on the right side shows the initial position of the particle. The particle follows the familiar cyclotron, bounce, and drift modes as it moves under the influence of the dipole field. This path simulates an energetic electron in the radiation belt region of the Earth’s magnetosphere.\nThe kinetic energy of a charged particle moving under a purely magnetic field must stay constant. We can use this fact to check the stability of the scheme.\n\nspeed = [np.sqrt((_**2).sum()) for _ in v_steps]\nprint(\"Range of speed:\", max(speed) - min(speed))\nplt.plot(speed)\nplt.grid()\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Speed\");\n\nRange of speed: 1.0325074129013956e-14\n\n\n\n\n\n\n\n\n\nThe speed (magnitude of velocity) is extremely stable, as expected, despite the fact that the direction of velocity changes rapidly due to the system’s dynamics. The difference between largest and smallest values is comparable to the machine precision."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-gravitational-acceleration",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-gravitational-acceleration",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Application: Gravitational acceleration",
    "text": "Application: Gravitational acceleration\nHere we apply the method to the Kepler problem, 2-dimensional motion of a test body under the influence of a large mass. Here, the acceleration does not depend on velocity, only to position, so the update step is simpler.\nNote that when the acceleration does not depend on the velocity, the scheme is not implicit anymore. The position and velocity updates can be combined in the following form:\n\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\tau\\mathbf{v}_n + \\frac{\\tau^2}{2} \\mathbf{a}(\\mathbf{x}_n+\\frac{\\tau}{2}\\mathbf{v}_n)\n\\] where all the terms on the right-hand side are calculated using values at step \\(n\\).\nThe gravitational acceleration of a body due to a mass \\(M\\) is\n\\[\n\\mathbf{a}(\\mathbf{x}) = -\\frac{GM}{|\\mathbf{x}|^3}\\mathbf{x}\n\\]\nwhere \\(G\\) is the gravitational constant. In the simulation, we will set \\(GM=10\\)1. The attracting mass is fixed at the origin.\n1 Physically this corresponds to a very small asteroid, with a mass of about 150 million tonnes and a diameter of 100-200 meters. With this choice we have convenient position and velocity values of the order of 1.\ndef update(x, v, dt, acceleration):\n    x1 = x + v*dt/2 # intermediate step\n    v_next = v + dt*acceleration(x1)\n    x_next = x + (v_next + v)*dt/2\n    return x_next, v_next\n\nGM = 10 # acceleration factor\ndef grav_acc(x):\n    rcube = ((x**2).sum())**(3/2)\n    return -GM/rcube * x\n\ndef iterate(x0, v0, dt, nsteps):\n    x = x0; v = v0;\n    x_steps = [x]\n    v_steps = [v]\n    t_steps = np.arange(0,nsteps*dt+dt,dt)\n    for step in range(nsteps):\n        x, v = update(x,v,dt,grav_acc)\n        x_steps.append(x)\n        v_steps.append(v)\n    x_steps = np.array(x_steps)\n    v_steps = np.array(v_steps)\n    return t_steps, x_steps, v_steps\n\nInitialize the position and velocity, set a time step of 0.2 seconds, and follow for 500 steps.\n\nx0 = np.array([4,0]) # initial position\nv0 = np.array([0,1.8])  # initial velocity\nt, x, v = iterate(x0, v0, dt=0.2, nsteps=500)\n\nVisualize the trajectory. The green arrow is the initial velocity, and the star at the origin indicates the attracting body.\n\nplt.plot(x[:,0], x[:,1], c=\"C1\")\nplt.arrow(x[0,0], x[0,1], dx=v[0,0], dy=v[0,1], width=0.1, ec=\"C2\", fc=\"C2\", head_width=0.5)\nplt.scatter(0,0,s=200,marker=\"*\",c=\"k\")\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.gca().set_aspect(\"equal\")\n\n\n\n\n\n\n\n\nUnlike the magnetic force, gravitational force does not keep the kinetic energy constant. However, energy conservation still applies. The sum of kinetic energy and the potential energy per mass should be constant:\n\\[\nE = \\frac{1}{2}|\\mathbf{v}|^2 - \\frac{GM}{|\\mathbf{x}|} = \\mathrm{constant}\n\\]\nChecking the energy from numerical solution tells us about the error of the algorithm:\n\ndef total_energy(x,v):\n    ke = 0.5*(v**2).sum(axis=1)\n    r = np.sqrt((x**2).sum(axis=1))\n    pe = -GM/r\n    return ke+pe\n\nplt.plot(t, total_energy(x,v))\nplt.title(\"Total energy per mass\")\nplt.xlabel(\"time\")\nplt.ylabel(\"Energy\")\nplt.grid();\n\n\n\n\n\n\n\n\nThe energy oscillates with a small amplitude, but there is no significant drift. With a smaller step size, the oscillations can be reduced:\n\nx0 = np.array([4,0]) # initial position\nv0 = np.array([0,1.8])  # initial velocity\n\nt, x, v = iterate(x0, v0, dt=0.2, nsteps=500)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.2, $\\\\Delta E=${delta_e:.2e}\")\n\nt, x, v = iterate(x0, v0, dt=0.1, nsteps=1000)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.1, $\\\\Delta E=${delta_e:.2e}\")\n\nt, x, v = iterate(x0, v0, dt=0.05, nsteps=2000)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.05, $\\\\Delta E=${delta_e:.2e}\")\n\nplt.title(\"Total energy per mass\")\nplt.xlabel(\"time\")\nplt.ylabel(\"Energy\")\nplt.grid()\nplt.legend();\n\n\n\n\n\n\n\n\nFor every halving of the time step, the amplitude of oscillation \\(\\Delta E\\) is reduced by a factor of 4. Even though we did not do a formal analysis, this observation suggests that Michel’s method is a second-order scheme, where the error reduces as the square of the step size: \\(\\epsilon \\sim \\mathcal{O}(\\tau^2)\\)"
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html",
    "href": "posts/en/2025/set-up-data-science-projects/index.html",
    "title": "How to set up your data science projects",
    "section": "",
    "text": "Photo by Tai Bui on Unsplash\nYou are a data scientist beginning with a new task. You think it through, decide on the models you’re going to try, establish your data sources. You are ready to start coding.\nHow do you start?\nMaybe you launch a Jupyter notebook, import your libraries, and start hacking away. But, after you complete the task, what do you do with this huge notebook? Usually our work is in the middle of a chain of tasks. A machine-learning model or a data transformation module takes input from other software, sends output to some other piece of software.\nIf you structure your code in a more software-engineering-friendly way, these connections may become easier to establish. Also, your code and analyses will be better organized and reproducible.\nI come from an academic background. Programs I wrote in my academic career were limited in scope, and only to answer a specific question. When I switched careers, I had to think about how to set up a software project, and ask around. I know that there are quite a number of people like me, so I’d like to share some ideas that worked for me.\nHere are my steps for starting a data-science project:\nDepending on your work flow, there may be other steps, such as setting up a Dockerfile to dockerize your project, or configuration files for cloud services like AWS. I won’t discuss them here.\nI use Linux, and the following examples use Linux terminal. I’m unable to describe these steps in other operating systems. If you use Windows, you should look up the exact commands for the tasks described here."
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html#what-is-a-project",
    "href": "posts/en/2025/set-up-data-science-projects/index.html#what-is-a-project",
    "title": "How to set up your data science projects",
    "section": "What is a “project”?",
    "text": "What is a “project”?\nEarly in my work as a data scientist, a seasoned SW engineer had told me “just use a virtual environment for each project”. That was puzzling for me. What is my project?\nFor me, the word “project” had a grand aura around it. A project would take a long time, maybe years. It would involve many people, led by a project manager. My daily work was far from that. I was just writing some Jupyter notebooks, producing source files, generating graphics for presentations. So, when my boss asks me to do a forecast of sales, how is that a “project”?\nMy confusion cleared up when I realized that a project doesn’t have to be a big thing, like a skyscraper, or an operating system. In fact, any task with a clear outcome and clear constraints is a project.\nSo, writing a program to forecast next year’s sales and generating a report is, indeed, a project. Creating a dashboard that queries the database daily and displays a graph of sales is another.\nYou need to specify when you will be done with a project. For example, “researching the applications of LLMs in our market” is not a project, because you can never say that research is done. When you take up on a task, think about what the outcome should be, and set the boundaries of your project accordingly.\nThe SMART criteria of goal-setting can be relevant here. I find them hard to follow to the letter, but they are useful guidelines. You should try to understand the spirit of them.\nDecide on what the outcome of the project should be. With that decided, you know where to go and, more importantly, when to stop. Personally I have a hard time wrapping up projects, so I try to put clear stopping criteria up front."
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html#the-project-directory",
    "href": "posts/en/2025/set-up-data-science-projects/index.html#the-project-directory",
    "title": "How to set up your data science projects",
    "section": "The project directory",
    "text": "The project directory\nSoftware engineers advise having a dedicated directory for every project. I think this is useful. The project directory holds not only the source code, but other relevant files too, such as data, configuration files, documentation, or the environment. This means, you should not put all your code and scripts in a single big directory. The project directory must contain only files that are within the scope of the project.\nHere’s a typical project directory and subdirectories that I create when starting a project:\nmy_project\n├── notebooks\n├── README.md\n├── requirements.txt\n├── scripts\n├── src\n└── pyproject.toml\nHere src holds some source code associated with the project. Jupyter notebooks live in notebooks and they import code from src.\nInitially, you can work with Jupyter notebook for developing your code. However, as the algorithms mature, you should refactor the code into a reusable module and put it into src. This way you can avoid repeating yourself and having several versions of the code, which may cause confusion later on. More importantly, you will develop a code collection that you may import in your scripts, notebooks, and other programs.\nscripts may or may not be necessary. This directory holds some programs that import modules from src. These can be used for repeating tasks, such as generating a report, or downloading data.\nrequirements.txt holds a list of packages installed in your virtual environment. It is necessary to recreate the environment when necessary. We discuss virtual environments below in more detail.\nREADME.md is a Markdown-formatted text file that stores a description of your project. This is the standard name used by GitHub to display relevant information about your project.\npyproject.toml configures your project as an installable Python package. More on this in a follow-up post.\nDepending on your needs, you can add more. For example, a data directory for storing raw data, Dockerfile for dockerizing your project, a models directory for trained and pickled models, or a reports directory to store generated reports.\n\nWhat about one-time analyses?\nMore often than not, a data scientist generates an ad-hoc analysis for a specific question: Detailed breakdown of sales in a specific branch, effect of weather on the use of our service, estimated revenue loss due to power outage in a store, etc.\nWork like these do not require their own directory, virtual environment, or GitHub repository. Instead, I put them into a single directory named adhoc_reports. This directory has its own virtual environment, and its own repository. Essentially, I treat the collection of reports as a single project.\nHere is a sample directory:\nadhoc_reports\n├── adhoc\n│   ├── __init__.py\n│   └── utils.py\n├── html\n│   ├── 2025-01-03 Sales Report.html\n│   ├── 2025-01-04 Sales Report.html\n│   └── 2025-01-04 Customer Analysis.html\n├── notebooks\n│   ├── 2025-01-01 My awesome analysis.ipynb\n│   └── 2025-01-02 Some visualizations.ipynb\n├── README.md\n├── requirements.txt\n├── scripts\n│   ├── generate_sales_report.sh\n│   └── generate_customer_analysis.sh\n└── pyproject.toml\n\n\n\n\n\n\nWhat about cookiecutter-datascience?\n\n\n\nMany people use the cookiecutter-datascience template to set up a data-science project. It is popular among developers and data scientists, and I used in a few projects of mine. It incorporates a lot of good ideas, guiding you toward better automation of data collection and processing. I strongly recommend everyone to look through the template, read the Opinions and understand the structure thoroughly.\nPersonally, I find this template difficult to use and maintain. It has many nested subdirectories, it is a hassle to navigate through them, and the purpose of each directory is not always clear.\ncookiecutter-datascience might be useful once your project reaches a certain level of sophistication; keep it in your radar. However, for projects I have done so far, a simpler structure turned out to be sufficient."
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html#use-a-virtual-environment",
    "href": "posts/en/2025/set-up-data-science-projects/index.html#use-a-virtual-environment",
    "title": "How to set up your data science projects",
    "section": "Use a virtual environment",
    "text": "Use a virtual environment\nA virtual environment (venv for short) is a collection of packages in your project directory. When you use a virtual environment, you launch a copy of the interpreter in that collection, instead of the one in the system. This interpreter then uses only the packages in the environment. Effectively, a virtual environment is an isolated Python installation.\nAfter creating the project directory, I go into it in the command shell, and create a venv there:\n$ python -m venv .venv\nThis command runs the Python module venv and creates a set of files under the .venv subdirectory. This subdirectory contains the library modules, executables, and other files necessary for a full-fledged Python system.\n$ tree -L1 .venv\n.venv\n├── bin\n├── etc\n├── include\n├── lib\n├── lib64 -&gt; lib\n├── pyvenv.cfg\n└── share\nThe name of the environment subdirectory is arbitrary. You can name it venv, projectenv, or anything else you want. I prefer .venv because the preceding dot makes this a hidden directory, That way it does not clutter the directory listing.\nAfter that, you need to activate the virtual environment.\n$ source .venv/bin/activate\nThis command usually changes the command prompt to include the environment name, so you’ll know which environment you’re using.\n(.venv) $\nWith the environment active, you can begin installing packages with pip\n(.venv) $ pip install ipykernel numpy pandas scipy matplotlib\nI use an alternative method to install modules. I prepare a file named requirements.txt storing the names of all the modules I want to install, one in every line.\n# requirements.txt\nipykernel\nnumpy\npandas\nscipy\nmatplotlib\nThen, all I need is to run the following command:\n(.venv) $ pip install -r requirements.txt\nand pip installs the modules all at once.\nWhenever I need to install a new package, I just add it as a new line in requirements.txt, and run the command again.\nAs a general rule, virtual environments must be regarded as disposable. With a requirements.txt file in place, you don’t need to remember which packages you’ve installed. You can create the entire environment with a single command.\nInitially, I didn’t like having a separate environment for every project. It seemed like an unnecessary duplication of packages. I used a single global environment for all tasks. That caused two types of problems for me:\n\nWhen you add a new package, the package installer (pip or conda, or whatever you use) needs to check its compatibility against all existing packages. If you have too many packages, this may take a long time. The more packages you have, the higher is the probability of non-compatibility.\nSome packages may require an older version of some existing packages and remove the newer versions. That may cause trouble with your existing programs relying on the newer version.\n\nIn time, my global environment became a mess. I realized that a local virtual environment is the best solution.\nIf you are concerned about the disk space, you can just delete the .venv directory of projects you don’t use. When you need to, you can always recreate it with:\n$ python -m venv .venv; source .venv/bin/activate; pip install -r requirements.txt\nThese are just the basic steps I use in a typical setup. For more details on virtual environments, see Python Virtual Environments: A Primer by Martin Breuss on RealPython."
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html#push-to-a-github-repository",
    "href": "posts/en/2025/set-up-data-science-projects/index.html#push-to-a-github-repository",
    "title": "How to set up your data science projects",
    "section": "Push to a Github repository",
    "text": "Push to a Github repository\ngit is the currently accepted standard tool for version control of software projects, and GitHub is the most common web platform for sharing software projects.\ngit allows you to keep track of all changes in your code, and revert them to a previous state, if necessary. More importantly, it is designed for collaborative software development. When people work on different parts of a project and combine their work, they may end up with many problems, inconsistencies or clashes. git does not fix the problems for you, but it helps you to correct them, or completely avoid them.\nIf you are just starting out and working by yourself, you might find git unnecessary. Still, some understanding of git is valuable even if you are not in a software team:\n\nYou are going to use git eventually, so you better make yourself familiar with it.\nPublishing your project on GitHub makes your work visible.\nGithub is a free backup service for your project. If you delete your project from your laptop, you can download it again anytime.\nIt can be used for other collaborative works, such as an academic research paper.\nThere are many services built around GitHub. You can use Binder to run your notebooks interactively in the cloud, or you can use Github Pages to host a static web site for free.\n\nInitialize git in your project folder\n$ git init\n$ git branch -m main\nThe second command renames the current branch to main. The default name is master, but this is considered offensive now, and GitHub uses main by default.\nCreate a file named .gitignore in your project file. This is a simple text file that holds the list of files and directories that should not be tracked by git. In particular, we want to ignore .venv directory. This one holds the packages installed for the virtual environment. It is too large, with hundreds of files. This is an unnecessary burden for git, because the virtual environment can easily be recreated.\n$ echo /.venv/ &gt; .gitignore\nAdd all your files and folders (except for those in .gitignore) to the staging area:\n$ git add .\nNow you can create a snapshot of your files which git will store. This is called a commit. Git requires a message with every commit, which summarizes the nature of the latest change.\n$ git commit -m \"first commit\"\nNow you have a local source-control system on your computer. As you develop your code and add new files, you can repeat the commands above to update the repository.\nUsually you want to synchronize this repository with the web, though. This is what GitHub is for.\nCreate a repository on the GitHub web page. You’ll need to create an account, if you don’t have one already. Just fill in the “Repository name” and click “Create repository”\n\nNow connect your local repository to the remote repository with the command\n$ git remote add origin &lt;repo-url&gt;\nFinally, synchronize the current git state with the remote repository:\n$ git push origin main\nWhen you visit the GitHub page, you’ll see that the files are updated in the remote repository.\n\n\n\n\n\n\nCaution\n\n\n\nIf your remote repository is created with some files (such as README.md or .gitignore) with different contents, this command will cause an error saying “fatal: Need to specify how to reconcile divergent branches.”\nTo fix this, first run the command:\n$ git pull origin main --rebase\nfollowed by\n$ git push origin main\n\n\nIf a version of a file is more recent in the remote repository, you pull it to your local repository with:\n$ git pull origin main\ngit is a very powerful tool. Most programmers use only a small subset of its abilities. A web search for “git for beginners” will return many resources. Choose one that is suitable for you.\n\n\n\n\n\n\nTip\n\n\n\nMany common IDEs such as VSCode or PyCharm have plug-ins for git. With these, you can perform most common source-control tasks by clicking on menus. You still need to know the concepts, though."
  },
  {
    "objectID": "posts/en/2025/set-up-data-science-projects/index.html#go-ahead",
    "href": "posts/en/2025/set-up-data-science-projects/index.html#go-ahead",
    "title": "How to set up your data science projects",
    "section": "Go ahead",
    "text": "Go ahead\nBasic steps described in this post should be a flexible starting point. Go ahead and develop your code. You don’t need a perfect setup. If you realize you need more fine-grained organization, do it as you go.\nThere is one more optional step that I find useful: Set up your project as an installable Python package. This is not essential for a minimalist setup, and it deserves it own follow-up post."
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "",
    "text": "Python’la yapay öğrenme modelleri çalıştıran her veri bilimcinin aşina olduğu bir kütüphanedir scikit-learn. Son derece zengin bir algoritma koleksiyonunu barındırır. Çoğu zaman o algoritmaları olduğu gibi kullanmak yeterlidir.\nAma bazen çok özelleşmiş bir algoritmaya ihtiyaç duyabilirsiniz. Bunu scikit-learn arayüzüne uygun şekilde yazarsanız, kütüphanenin sağladığı birçok başka kolaylığa da rahatça erişebilirsiniz.\nBu yazıda, kendi regresyon veya sınıflandırıcı modelinizi nasıl scikit-learn modülünün parçasıymış gibi yazabileceğinizi anlatacağım. Önce, scikit-learn sisteminin nasıl kullanıldığına bir bakalım."
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#hareketli-ortalama",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#hareketli-ortalama",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Hareketli ortalama",
    "text": "Hareketli ortalama\nÖrnek olarak, verilerin hareketli ortalamasını veren bir dönüştürücü hazırlayalım. Böyle bir işlem için bir dönüştürücü yazmak aşırı gelebilir, ama yararlı olacağı durumlar vardır. Sözgelişi, modelin eğitim kümesini hazırlarken bir adımda hareketli ortalama alıyor olabiliriz. Önceden bu işlemi bir dönüştürücü haline getirirsek, bütün eğitim ve kestirim sürecini bir pipeline içine koymamız, çapraz doğrulamaları aksamadan yapmamız mümkün olur.\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\n\nclass RollingMean(BaseEstimator, TransformerMixin):\n    def __init__(self, n=15):\n        self.n = n\n    \n    def moving_average_(self, a):\n        return np.convolve(a, np.ones(self.n), \"valid\") / self.n\n    \n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        return self\n    \n    def transform(self, X):\n        check_is_fitted(self, 'n_features_in_')\n        X = check_array(X)\n        # Moving average will have len(X) - n + 1 rows. For consistent size, fill the first n-1 rows with NaN.\n        empty_rows = np.array([np.nan]*(X.shape[1]*(self.n-1))).reshape(self.n-1,X.shape[1])\n        movav = np.apply_along_axis(self.moving_average_, axis=0, arr=X)\n        return np.r_[empty_rows, movav]\n\nBu örnekte fit() herhangi bir işlem yapmıyor, sadece girdi için doğruluk denetimi yapıyor ve n_features_in_ nesne özelliğini yaratıyor. Daha sonra transform() çağrıldığında bu değişkenin var olup olmadığı denetleniyor.\nÜç değişkenli bir rastgele yürüyüş verisi üretelim:\n\nnp.random.seed(29101923)\nX = np.cumsum(np.random.rand(30).reshape(3,-1).T, axis=0)\nX\n\narray([[0.74977745, 0.96371498, 0.32437245],\n       [1.71830174, 1.4121694 , 0.50916844],\n       [2.10912041, 1.54142327, 0.77301929],\n       [2.12452587, 1.97102509, 1.23124119],\n       [2.34991983, 2.00229673, 1.87464155],\n       [2.75549879, 2.71351979, 2.71059991],\n       [2.87756103, 3.29451338, 3.27770838],\n       [3.64506605, 3.47371139, 3.9280419 ],\n       [4.18478839, 4.08826894, 4.22898858],\n       [5.0129616 , 4.10754193, 4.32410028]])\n\n\n\nRollingMean(n=3).fit(X).transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])\n\n\nBu örnekte fit() ayrı bir iş yapmadığı için, aynı işlem doğrudan fit_transform() ile de yapılabilir. Bu metot TransformerMixin sınıfından miras alınır, o yüzden bizim açıkça eklememize lüzum yok.\n\nRollingMean(n=3).fit_transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])"
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#minimum-maksimum-ölçekleme",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#minimum-maksimum-ölçekleme",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Minimum-maksimum ölçekleme",
    "text": "Minimum-maksimum ölçekleme\nBaşka bir örnek olarak, minimum-maksimum arası ölçekleme için bir dönüştürücü oluşturalım1. Bu örnekte fit() metodu boş durmuyor, eğitim kümesinin en büyük ve en küçük değerlerini bulup bir kenara yazıyor.\n1 Bunun için hazır bir dönüştürücü var ama örnek için yokmuş gibi yapalım.\nclass MinmaxScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        self.max_ = X.max(axis=0)\n        self.min_ = X.min(axis=0)\n        return self\n    def transform(self, X):\n        check_is_fitted(self, \"max_\")\n        X = check_array(X)\n        return (X - self.min_) / (self.max_ - self.min_)\n\nBurada ölçekleme için herhangi bir parametre alınmadığından başlatıcı __init__ boş kalıyor (ama Python sentaksı gereği sınıf tanımında bulunmak zorunda).\nBu dönüştürücünün bir öncekinden farkı, eğitim kümesine bağlı oluşu. Bu ölçekleyici, eğitim verisinin sütunlarının minimum ve maksimum değerlerini belirliyor. transform() işleminde ise, aldığı verileri minimum değer 0 ve maksimum değer 1 olacak şekilde lineer bir fonksiyonla dönüştürüyor.\nBu dönüştürücüyü yine rastgele üretilmiş verilerle deneyelim:\n\nX_train = np.random.rand(20,3)\nX_test = np.random.rand(10,3)\nX_test\n\narray([[0.35628925, 0.66850959, 0.84267923],\n       [0.69403501, 0.38350617, 0.62806853],\n       [0.38789683, 0.53202186, 0.61269947],\n       [0.51598942, 0.37764728, 0.36217848],\n       [0.02539575, 0.26381172, 0.64366218],\n       [0.91755454, 0.36221054, 0.50155528],\n       [0.09683517, 0.71776499, 0.7394225 ],\n       [0.81163167, 0.49544736, 0.63947337],\n       [0.58945327, 0.50590908, 0.03767375],\n       [0.09311029, 0.12597857, 0.72295531]])\n\n\n\nscaler = MinmaxScaler().fit(X_train)\nscaler.transform(X_test)\n\narray([[ 3.42904840e-01,  6.06674521e-01,  8.79083858e-01],\n       [ 7.17503869e-01,  2.46825035e-01,  6.44693329e-01],\n       [ 3.77961291e-01,  4.34343120e-01,  6.27907771e-01],\n       [ 5.20030762e-01,  2.39427514e-01,  3.54297244e-01],\n       [-2.40942497e-02,  9.56970773e-02,  6.61724192e-01],\n       [ 9.65412838e-01,  2.19936867e-01,  5.06519853e-01],\n       [ 5.51403047e-02,  6.68865111e-01,  7.66310361e-01],\n       [ 8.47932141e-01,  3.88163622e-01,  6.57149313e-01],\n       [ 6.01510650e-01,  4.01372740e-01, -1.15815402e-04],\n       [ 5.10089822e-02, -7.83330700e-02,  7.48325452e-01]])\n\n\nDönüştürücümüz check_estimator testlerinden de başarıyla geçiyor.\n\ncheck_estimator(MinmaxScaler())"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaan Öztürk",
    "section": "",
    "text": "📩 Email\n  \n  \n    \n     Twitter\n  \n  \n     Bluesky\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Github\n  \n  \n    \n     Google Scholar\n  \n\n  \n  \nI’m a data scientist based in Istanbul and Tekirdağ, where I share my life with my family and our beloved cats.\nMy training is in physics, culminating with a PhD in magnetospheric physics. For over a decade, I taught at various universities, designing and leading programming courses and guiding students and teaching assistants alike. Eventually, I transitioned from academia to pursue an exciting career in data science.\nI’m a proud alumnus of İstanbul Erkek Lisesi (1991), Boğaziçi University (B.Sc. 1996, M.Sc 1999), and Rice University (PhD 2004).\nThis site is mainly about my technical notes. For the interested, I have written a number of pieces in other websites (all in Turkish):\n\nYalansavar: Scientific skepticism\nAçık Bilim: Popular science\nVeri Defteri: Data science, machine learning, programming.\nBirGün Gazetesi: Biweekly column on scientific skepticism\nWordpress Blog: My other personal blog"
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Kaan Öztürk",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nHow to set up your data science projects\n\n\nTips from a minimalist perspective\n\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMy lecture notes on R\n\n\nand some rants about teaching the course\n\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating the probability of rare events\n\n\n“It is most unlikely. But—here comes the big ‘but’—not impossible.” – Roald Dahl\n\n\n\nNov 29, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\nAll Posts »"
  }
]