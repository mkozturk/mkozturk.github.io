[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak\n\n\n\n\n\n\n\ndata science\n\n\n\n\nAlgoritmalarınızı scikit-learn yapısına uygun olarak yazın ve bu zengin kütüphanenin gücünü tam olarak kullanın\n\n\n\n\n\n\nNov 25, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\n  \n\n\n\n\nSeçim Sandığında Bayes\n\n\n\n\n\n\n\nBayesian analysis\n\n\n\n\nArdışık oy uzunluğuyla sonuç tahmini\n\n\n\n\n\n\nJun 10, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "title": "Seçim Sandığında Bayes",
    "section": "",
    "text": "28 Mayıs’daki cumhurbaşkanlığı seçiminde oyların sayılmasını izlerken bir şey dikkatimi çekti: Bulunduğum sandıkta Kemal Kılıçdaroğlu’na (KK) arka arkaya çok sayıda oy çıkarken, Recep Tayyip Erdoğan’a (RTE) ardışık olarak ikiden fazla oy çıkmıyordu. Yani bir veya iki oy RTE, ardından KK oyları, ardından bir veya iki RTE oyu, vs.\nBunun üzerine aklıma şu soru geldi: Sandıkta bir adayın nihai oy oranını, sayım sırasında o aday için gördüğümüz en uzun ardışık oy uzunluğuna dayanarak (oyların anlık sayısını görmediğimizi varsayarak) tahmin edebilir miyiz?\nBu anlamlı bir soru, çünkü o sandıkta adayın (bilinmeyen) oy oranı yüksekse, en uzun zincirin uzunluğu da ona göre artacaktır. Sıfıra yakın bir olasılıksa uzunluk biri aşmayacak, yüzde yüze yakınsa da sayılan oy sayısına yakın olacak.\nBu azami zincir uzunluğu kaç oyun sayıldığına da bağlı. Çok çok sayıda oy varsa, büyük sayılar yasası gereği düşük oy oranında bile herhangi bir uzunlukta zincir görmek mümkün. Tersten bakarsak, azami zincir uzunluğunu ikide sabitlemekle, oy sayısı arttıkça oy oranı tahminimiz sıfıra yaklaşmak zorunda kalacak. Sandıklarda sadece birkaç yüz oy olduğu için bu sınırlara yaklaşmayacağız tabii. Yine de bu aşırı durumları akılda tutmak sonuçlarımızı kontrol etmek için yararlı.\nSandıkta adayın oy oranına \\(r\\) diyelim. Bunu bilmiyoruz, kestirmek istiyoruz. Sandıkta toplam 352 oy var. Bunların tamamı açıldıktan sonra tahmin edecek bir şey kalmıyor. Belli bir sayıda, mesela 100 oy açıldıktan sonra sonucu tahmin etmek istiyoruz.\nİlk aşamada, 100 oy içinde adayın arka arkaya aldığı oy zincirlerinin en uzununun 2 olması olasılığını bulalım.\nBu olasılık belki analitik yoldan bulunabilir ama ben yapamadım, o yüzden rastgele sayı üreten kısa bir program yazdım:\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import groupby\n\ndef oylar_üret(r, oy_sayısı):\n    return random.choices((\"RTE\",\"KK\"), weights=[r, 1-r], k=oy_sayısı)\n\ndef en_uzun_dizi_uzunluğu(x, değer=\"RTE\"):\n    return max(len(list(y)) for c,y in groupby(x) if c==değer)\n    \ndef olabilirlik(r, oy_sayısı, deneme=1000, gözlenen_uzunluk=2):\n    sayaç = 0\n    for i in range(deneme):\n        oylar = oylar_üret(r, oy_sayısı)\n        try:\n            if en_uzun_dizi_uzunluğu(oylar) == gözlenen_uzunluk:\n                sayaç += 1\n        except ValueError: # birinci seçenek hiç gözlenmediyse\n            continue\n    return sayaç / deneme\nBirinci fonksiyon oylar_üret(r, oy_sayısı) iki adaylı bir seçimde, birinci adayın oy oranı r olacak şekilde oy_sayısı kadar rastgele oy üretir. İkinci fonksiyon ise, birincinin çıktısını alıp onun içinde ardışık değer zincirinin en büyük uzunluğunu bulur.\nÖrnek:\nrandom.seed(20232023)\ns = oylar_üret(r=0.2, oy_sayısı=10)\ns\n\n['KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'RTE', 'RTE', 'KK']\nen_uzun_dizi_uzunluğu(s, \"RTE\")\n\n2\nÜçüncü fonksiyon olabilirlik, en uzun dizi uzunluğunun tam 2 olduğu durumların olasılığını, aynı oy oranı ve oy sayısıyla birçok rastgele sandık üreterek kestirir.\nÖrneğin, oy oranı 0.2 ise ve 10 adet oy açıldıysa, tam 2 uzunlukta en az bir dizi olması olasılığı yaklaşık 0.22 olur.\nolabilirlik(0.2, oy_sayısı=10, deneme=10000, gözlenen_uzunluk=2)\n\n0.2184\nTabii biz asıl oy oranını bilmiyoruz, o yüzden tersten gideceğiz: Farklı oy oranları için, 100 oy içinde tam 2 uzunlukta en az bir dizi görme olabilirliğini hesaplayacağız.\nCode\nr = np.linspace(0, 1, 101)\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nplt.plot(r, L)\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Olabilirlik\")\nplt.grid()\n\n\n\n\n\nOy oranına göre, iki uzunlukta en az bir dizi görme olasılığı\nBu sandıkta, RTE’nin oy oranının olasılık dağılımının 0.5’e doğru neredeyse sıfıra indiğini görüyoruz. Dağılımın tepesi 0.15 değerinde. Dağılım geniş bir aralığa yayılmış olduğu için sonuç hakkında net bir şey söylemek şimdilik zor olsa da, yarıyı geçme şansının çok düşük olduğu anlaşılıyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "title": "Seçim Sandığında Bayes",
    "section": "Bayes formülü",
    "text": "Bayes formülü\nBu noktada biraz duraklayıp bu problemi bir bayesçi kestirim alıştırması olarak ifade edelim. Oy oranı \\(r\\) ise ve \\(G\\) gözlemimizi (yani en uzun ardışık zincirin 2 uzunlukta olmasını) temsil ediyorsa, bu gözlem verilmiş olarak oy oranının olasılık dağılımını Bayes formülüyle ifade ederiz:\n\\[P(r | G) = \\frac{1}{P(G)}P(G|r)P(r)\\]\nÖnsel inanç: Burada \\(P(r)\\), bir şey gözlemeden önceki oy oranının dağılımıdır. Hiç bir şey gözlemediysek bunu nasıl bilebiliriz? Geçmiş tecrübelerimize dayanarak bir fikrimiz olabilir, veya her şey olabilir diyerek 0-1 arasında düzgün bir dağılım varsayabiliriz. Bu dağılıma önsel inanç (“prior belief”) denir.\nOlabilirlik: \\(P(G|r)\\) ifadesi, belli bir oy oranı \\(r\\) ile, gözlemimizin olasılığını verir. Yukarıdaki olabilirlik fonksiyonu tam bunu yapar. Bu faktörün teknik adı da zaten olabilirlik (“likelihood”).\nOlabilirlik fonksiyonu ile sonuçların nasıl üretildiğine dair modelimizi analizin içine katarız. Mesela burada, iki seçenekten birinin rastgele seçildiği bir Bernoulli modeli kullandık. Başka problemlerde başka modeller kullanılması gerekecektir.\nOlabilirlik, aslında aradığımız şeyin tersidir: Parametrelerin bilinen değeriyle çıktılar üretir. Oysa biz bu çıktılardan yola çıkarak parametrenin ne olduğuna dair bir fikir edinmek istiyoruz. Yukarıdaki Bayes formülü bu ters problemi çözmemizi sağlar.\nNormalleştirme: \\(P(G)\\) ifadesi bir normalleştirme sabitidir ve şu şekilde hesaplanabilir: \\[P(G) = \\sum_{r} P(G|r)P(r)\\] Ama genellikle bu sabiti doğrudan kullanmaya gerek olmaz.\nSonsal inanç: Yukarıdaki faktörleri birleştirerek elde ettiğimiz \\(P(r|G)\\) olasılığına sonsal inanç (“posterior belief”) denir. Bu, önsel inancımızın eldeki veriyle güncellenerek düzeltilmiş halidir."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "title": "Seçim Sandığında Bayes",
    "section": "Düzgün önsel ile tahminler",
    "text": "Düzgün önsel ile tahminler\nŞimdi bunu problemimize uygulayalım. Düzgün dağılmış bir önsel alalım, yani RTE’nin bu sandıktaki oy oranının 0 ile 1 arasında eşit olasılıkla herhangi bir değerde olabileceğini düşünelim.\n\n\nCode\nr = np.linspace(0, 1, 101)\nprior = np.ones_like(r) \nprior /= sum(prior) # normalizasyon\n\n# her oy oranı için olabilirliği hesapla\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.grid()\n\n\n\n\n\nÖnsel (prior) dağılım ve 100 oyda an az bir tane 2 uzunlukta zincir olabilirliği ile sonsal (posterior) dağılım\n\n\n\n\nÖnceki grafiğin aynısı çıktı, ki sabit bir önsel aldığımız için böyle olması gerek (olabilirliği sabit bir sayıyla çarpıyoruz)\nBurada tek bir oy oranı tahmini çıkarmıyoruz. Bayesçi kestirim bize tek tahminler değil, tahmin edilecek değişken için bir olasılık dağılımı verir. İsterseniz bu dağılımdan tek tahminler (point estimate) çıkarabilirsiniz. Örneğin en yüksek olasılıklı değeri 0.15, ortalama değeri 0.17 olarak bulabiliriz.\nBaşka bir özet sayı, sonsal dağılımın ortasında %90 alanı kaplayan aralığın (“highest posterior density interval”) sınırlarıdır. Buna göre RTE’nin oy oranını, %90 olasılıkla (0.06, 0.29) arasında tahmin edebiliriz.\n\n\nCode\nprint(f\"En muhtemel değer: { r[np.argmax(posterior)] : .2f}\")\nprint(f\"Ortalama değer: { np.sum(r*posterior): .2f}\")\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\nEn muhtemel değer:  0.14\nOrtalama değer:  0.17\n%90 yoğunluk aralığı: 0.06  - 0.29"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "title": "Seçim Sandığında Bayes",
    "section": "Güncelleme",
    "text": "Güncelleme\nDiyelim 100 oy daha sayıldı ve RTE’ye yine en fazla 2 uzunlukta ardışık zincirler gözlediniz. Bu yeni veriyle RTE’nin oy oranına dair inancınızı (sonsal dağılımı) güncelleyebilirsiniz.\nBayesci analizin güzel tarafı, bunu yaparken sıfırdan başlamak zorunda olmamanız. Bir önceki adımda bulduğunuz sonsal dağılımı bu sefer önsel dağılım olarak kullanıp aynı işlemi tekrarlayabilirsiniz.\nOlabilirlik hesabını tekrarlamak zorunda değilsiniz, çünkü parametreler tamamen aynı olduğu için fonksiyon da aynı olacak.\n\n\nCode\nprior = posterior\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 200 oy\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.08  - 0.24\n\n\n\n\n\nYeni veri ile tahmin güncelleme\n\n\n\n\nBu yeni veri ile sonsal dağılım biraz daha daraldı. Güncellemeden sonra %90 yoğunluk aralığı 0.08-0.24 oldu."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı olabilirlikler",
    "text": "Farklı olabilirlikler\nSandıktaki oyların sayımı bittiğinde RTE’nin 352 oydan 83’ünü aldığı görüldü. Yani gerçekleşen oran 0.24 olmuş. Bu değer yukarıda bulduğumuz %90 aralığının tam sınırında. Sonsal dağılımımız olması gerekenin biraz altında kalmış görünüyor.\nBunun birkaç sebebi olabilir. Birincisi, ardışık oy dizisi uzunluğu iyi bir gösterge olmayabilir. İkincisi, benim gözlemim yanlış olabilir. Belki başlarda arka arkaya üç tane RTE oyu çıkmıştır da, ben o sırada ardışıklığa dikkat etmediğim için kaçırmış olabilirim.\nHesabı bu ihtimale göre iki aşamada tekrarlayalım. İlk 100 oy içinde en uzun RTE zinciri 3 uzunlukta olsun, ondan sonraki 100 oy içinde 2 uzunlukta olsun.\n\n\nCode\nprior1 = np.ones_like(r)\nprior1 /= sum(prior1)\n\nL1 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=3) for rr in r]\nposterior1 = L1*prior1\nposterior1 /= sum(posterior1) # normalizasyon\n\nprior2 = posterior1\nL2 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior2 = L2*prior2\nposterior2 /= sum(posterior2) # normalizasyon\n\nplt.plot(r, prior1, label=\"prior 1\")\nplt.plot(r, posterior1, label=\"posterior 1 (prior 2)\")\nplt.plot(r, posterior2, label=\"posterior 2\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior2))][0]}  - {r[(np.cumsum(posterior2)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.11  - 0.3\n\n\n\n\n\nİlk 100 oy içinde 3 uzunlukta en az bir zincir, ikinci 100 oy içinde 2 uzunlukta en az bir zincir gözlendiğinde sonsal dağılımlar\n\n\n\n\nİlk 100 oyda üçlü diziler olduğu için, birinci sonsal dağılımımız sağa kaymış, yani yüksek oy oranlarının olasılığı artmış. Bunun sonucu olarak da 200 oy gözlendikten sonraki sonsal, bir önceki çözümümüze göre daha sağa kaymış. Böylece %90 yoğunluk aralığı daha geniş ve gerçekleşen oranı içeriyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı önsel: Geçmiş seçimden bilgi aktarma",
    "text": "Farklı önsel: Geçmiş seçimden bilgi aktarma\nSon olarak, farklı bir önsel kullanmayı deneyelim.\nŞimdiye kadar kullandığımız önsel, bir aday için bütün oy oranlarının eşit olasılıkta olduğunu varsayıyordu. Oysa bunun doğru olmadığını geçmiş tecrübemizden biliyoruz. Seçimin birinci turunda aynı sandıkta 357 oyun 81’i RTE’ye gittiğini gözlemiştik. Bu bilgiyle, Beta dağılımına uyan bir önsel belirleyebiliriz:\n\\[P(r) = C\\ r^{81} (1-r)^{276}\\]\nYine, ilk 100 oyda en uzun zincirin 2 uzunlukta olduğunu varsayarak sonsal hesaplayalım.\n\n\nCode\nprior = r**81 * (1-r)**276\nprior /= sum(prior)\n\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior = L*prior\nposterior /= sum(posterior)\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.xlim((0.1,0.4))\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.19  - 0.25\n\n\n\n\n\nGeçmiş seçimin oy oranlarıyla oluşturulan önsel ve bununla elde edilen sonsal dağılım.\n\n\n\n\n100 oy sayımında en fazla 2 uzunlukta zincir görmekle, sonsal olasılık dağılımının hafifçe sola kaydığını, yani oy oranının daha düşük ihtimallere kaydığını görüyoruz.\nAyrıca, daha belirli (informative) bir önsel seçmekle, sonsal dağılımımızın %90 yoğunluk aralığı daraldı, yani belirsizliği daha az bir sonuç elde ettik."
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "",
    "text": "Python’la yapay öğrenme modelleri çalıştıran her veri bilimcinin aşina olduğu bir kütüphanedir scikit-learn. Son derece zengin bir algoritma koleksiyonunu barındırır. Çoğu zaman o algoritmaları olduğu gibi kullanmak yeterlidir.\nAma bazen çok özelleşmiş bir algoritmaya ihtiyaç duyabilirsiniz. Bunu scikit-learn arayüzüne uygun şekilde yazarsanız, kütüphanenin sağladığı birçok başka kolaylığa da rahatça erişebilirsiniz.\nBu yazıda, kendi regresyon veya sınıflandırıcı modelinizi nasıl scikit-learn modülünün parçasıymış gibi yazabileceğinizi anlatacağım. Önce, scikit-learn sisteminin nasıl kullanıldığına bir bakalım."
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html#hareketli-ortalama",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html#hareketli-ortalama",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Hareketli ortalama",
    "text": "Hareketli ortalama\nÖrnek olarak, verilerin hareketli ortalamasını veren bir dönüştürücü hazırlayalım. Böyle bir işlem için bir dönüştürücü yazmak aşırı gelebilir, ama yararlı olacağı durumlar vardır. Sözgelişi, modelin eğitim kümesini hazırlarken bir adımda hareketli ortalama alıyor olabiliriz. Önceden bu işlemi bir dönüştürücü haline getirirsek, bütün eğitim ve kestirim sürecini bir pipeline içine koymamız, çapraz doğrulamaları aksamadan yapmamız mümkün olur.\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\n\nclass RollingMean(BaseEstimator, TransformerMixin):\n    def __init__(self, n=15):\n        self.n = n\n    \n    def moving_average_(self, a):\n        return np.convolve(a, np.ones(self.n), \"valid\") / self.n\n    \n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        return self\n    \n    def transform(self, X):\n        check_is_fitted(self, 'n_features_in_')\n        X = check_array(X)\n        # Moving average will have len(X) - n + 1 rows. For consistent size, fill the first n-1 rows with NaN.\n        empty_rows = np.array([np.nan]*(X.shape[1]*(self.n-1))).reshape(self.n-1,X.shape[1])\n        movav = np.apply_along_axis(self.moving_average_, axis=0, arr=X)\n        return np.r_[empty_rows, movav]\n\nBu örnekte fit() herhangi bir işlem yapmıyor, sadece girdi için doğruluk denetimi yapıyor ve n_features_in_ nesne özelliğini yaratıyor. Daha sonra transform() çağrıldığında bu değişkenin var olup olmadığı denetleniyor.\nÜç değişkenli bir rastgele yürüyüş verisi üretelim:\n\nnp.random.seed(29101923)\nX = np.cumsum(np.random.rand(30).reshape(3,-1).T, axis=0)\nX\n\narray([[0.74977745, 0.96371498, 0.32437245],\n       [1.71830174, 1.4121694 , 0.50916844],\n       [2.10912041, 1.54142327, 0.77301929],\n       [2.12452587, 1.97102509, 1.23124119],\n       [2.34991983, 2.00229673, 1.87464155],\n       [2.75549879, 2.71351979, 2.71059991],\n       [2.87756103, 3.29451338, 3.27770838],\n       [3.64506605, 3.47371139, 3.9280419 ],\n       [4.18478839, 4.08826894, 4.22898858],\n       [5.0129616 , 4.10754193, 4.32410028]])\n\n\n\nRollingMean(n=3).fit(X).transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])\n\n\nBu örnekte fit() ayrı bir iş yapmadığı için, aynı işlem doğrudan fit_transform() ile de yapılabilir. Bu metot TransformerMixin sınıfından miras alınır, o yüzden bizim açıkça eklememize lüzum yok.\n\nRollingMean(n=3).fit_transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])"
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html#minimum-maksimum-ölçekleme",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/scikit-learn API'sine uyumlu tahminleyiciler yazmak.html#minimum-maksimum-ölçekleme",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Minimum-maksimum ölçekleme",
    "text": "Minimum-maksimum ölçekleme\nBaşka bir örnek olarak, minimum-maksimum arası ölçekleme için bir dönüştürücü oluşturalım1. Bu örnekte fit() metodu boş durmuyor, eğitim kümesinin en büyük ve en küçük değerlerini bulup bir kenara yazıyor.1 Bunun için hazır bir dönüştürücü var ama örnek için yokmuş gibi yapalım.\n\nclass MinmaxScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        self.max_ = X.max(axis=0)\n        self.min_ = X.min(axis=0)\n        return self\n    def transform(self, X):\n        check_is_fitted(self, \"max_\")\n        X = check_array(X)\n        return (X - self.min_) / (self.max_ - self.min_)\n\nBurada ölçekleme için herhangi bir parametre alınmadığından başlatıcı __init__ boş kalıyor (ama Python sentaksı gereği sınıf tanımında bulunmak zorunda).\nBu dönüştürücünün bir öncekinden farkı, eğitim kümesine bağlı oluşu. Bu ölçekleyici, eğitim verisinin sütunlarının minimum ve maksimum değerlerini belirliyor. transform() işleminde ise, aldığı verileri minimum değer 0 ve maksimum değer 1 olacak şekilde lineer bir fonksiyonla dönüştürüyor.\nBu dönüştürücüyü yine rastgele üretilmiş verilerle deneyelim:\n\nX_train = np.random.rand(20,3)\nX_test = np.random.rand(10,3)\nX_test\n\narray([[0.35628925, 0.66850959, 0.84267923],\n       [0.69403501, 0.38350617, 0.62806853],\n       [0.38789683, 0.53202186, 0.61269947],\n       [0.51598942, 0.37764728, 0.36217848],\n       [0.02539575, 0.26381172, 0.64366218],\n       [0.91755454, 0.36221054, 0.50155528],\n       [0.09683517, 0.71776499, 0.7394225 ],\n       [0.81163167, 0.49544736, 0.63947337],\n       [0.58945327, 0.50590908, 0.03767375],\n       [0.09311029, 0.12597857, 0.72295531]])\n\n\n\nscaler = MinmaxScaler().fit(X_train)\nscaler.transform(X_test)\n\narray([[ 3.42904840e-01,  6.06674521e-01,  8.79083858e-01],\n       [ 7.17503869e-01,  2.46825035e-01,  6.44693329e-01],\n       [ 3.77961291e-01,  4.34343120e-01,  6.27907771e-01],\n       [ 5.20030762e-01,  2.39427514e-01,  3.54297244e-01],\n       [-2.40942497e-02,  9.56970773e-02,  6.61724192e-01],\n       [ 9.65412838e-01,  2.19936867e-01,  5.06519853e-01],\n       [ 5.51403047e-02,  6.68865111e-01,  7.66310361e-01],\n       [ 8.47932141e-01,  3.88163622e-01,  6.57149313e-01],\n       [ 6.01510650e-01,  4.01372740e-01, -1.15815402e-04],\n       [ 5.10089822e-02, -7.83330700e-02,  7.48325452e-01]])\n\n\nDönüştürücümüz check_estimator testlerinden de başarıyla geçiyor.\n\ncheck_estimator(MinmaxScaler())"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaan Öztürk",
    "section": "",
    "text": "I am a senior data scientist, currently working at Kavaken, developing predictive analytics tools for wind turbines. I live in Istanbul and Tekirdağ with my family and our three cats.\nMy training is in physics, with a PhD in magnetospheric physics. I have taught in various universities for more than a decade, before leaving academia altogether for a career in data science.\nI’m a proud alumnus of İstanbul Erkek Lisesi, Boğaziçi University, and Rice University.\nThis site is mainly about my technical notes. For the interested, I have written a number of pieces in other websites (all in Turkish):\n\nYalansavar: Scientific skepticism\nAçık Bilim: Popular science\nVeri Defteri: Data science, machine learning, programming.\nBirGün Gazetesi: Biweekly column on scientific skepticism\nWordpress Blog: My other personal blog"
  }
]