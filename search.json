[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Michel’s method for numerical solution of equations of motion\n\n\n\n\n\n\n\nnumerical methods\n\n\ncomputational physics\n\n\n\n\nA simple but powerful method for solving equations of motion, developed by the late Prof. Curtis Michel.\n\n\n\n\n\n\nNov 22, 2024\n\n\nKaan Öztürk\n\n\n\n\n\n\n  \n\n\n\n\nScikit-learn ile Kendi Tahminleyicilerimizi Yazmak\n\n\n\n\n\n\n\ndata science\n\n\n\n\nAlgoritmalarınızı scikit-learn yapısına uygun olarak yazın ve bu zengin kütüphanenin gücünü tam olarak kullanın\n\n\n\n\n\n\nNov 25, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\n  \n\n\n\n\nSeçim Sandığında Bayes\n\n\n\n\n\n\n\nBayesian analysis\n\n\n\n\nArdışık oy uzunluğuyla sonuç tahmini\n\n\n\n\n\n\nJun 10, 2023\n\n\nKaan Öztürk\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html",
    "title": "Seçim Sandığında Bayes",
    "section": "",
    "text": "28 Mayıs’daki cumhurbaşkanlığı seçiminde oyların sayılmasını izlerken bir şey dikkatimi çekti: Bulunduğum sandıkta Kemal Kılıçdaroğlu’na (KK) arka arkaya çok sayıda oy çıkarken, Recep Tayyip Erdoğan’a (RTE) ardışık olarak ikiden fazla oy çıkmıyordu. Yani bir veya iki oy RTE, ardından KK oyları, ardından bir veya iki RTE oyu, vs.\nBunun üzerine aklıma şu soru geldi: Sandıkta bir adayın nihai oy oranını, sayım sırasında o aday için gördüğümüz en uzun ardışık oy uzunluğuna dayanarak (oyların anlık sayısını görmediğimizi varsayarak) tahmin edebilir miyiz?\nBu anlamlı bir soru, çünkü o sandıkta adayın (bilinmeyen) oy oranı yüksekse, en uzun zincirin uzunluğu da ona göre artacaktır. Sıfıra yakın bir olasılıksa uzunluk biri aşmayacak, yüzde yüze yakınsa da sayılan oy sayısına yakın olacak.\nBu azami zincir uzunluğu kaç oyun sayıldığına da bağlı. Çok çok sayıda oy varsa, büyük sayılar yasası gereği düşük oy oranında bile herhangi bir uzunlukta zincir görmek mümkün. Tersten bakarsak, azami zincir uzunluğunu ikide sabitlemekle, oy sayısı arttıkça oy oranı tahminimiz sıfıra yaklaşmak zorunda kalacak. Sandıklarda sadece birkaç yüz oy olduğu için bu sınırlara yaklaşmayacağız tabii. Yine de bu aşırı durumları akılda tutmak sonuçlarımızı kontrol etmek için yararlı.\nSandıkta adayın oy oranına \\(r\\) diyelim. Bunu bilmiyoruz, kestirmek istiyoruz. Sandıkta toplam 352 oy var. Bunların tamamı açıldıktan sonra tahmin edecek bir şey kalmıyor. Belli bir sayıda, mesela 100 oy açıldıktan sonra sonucu tahmin etmek istiyoruz.\nİlk aşamada, 100 oy içinde adayın arka arkaya aldığı oy zincirlerinin en uzununun 2 olması olasılığını bulalım.\nBu olasılık belki analitik yoldan bulunabilir ama ben yapamadım, o yüzden rastgele sayı üreten kısa bir program yazdım:\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom itertools import groupby\n\ndef oylar_üret(r, oy_sayısı):\n    return random.choices((\"RTE\",\"KK\"), weights=[r, 1-r], k=oy_sayısı)\n\ndef en_uzun_dizi_uzunluğu(x, değer=\"RTE\"):\n    return max(len(list(y)) for c,y in groupby(x) if c==değer)\n    \ndef olabilirlik(r, oy_sayısı, deneme=1000, gözlenen_uzunluk=2):\n    sayaç = 0\n    for i in range(deneme):\n        oylar = oylar_üret(r, oy_sayısı)\n        try:\n            if en_uzun_dizi_uzunluğu(oylar) == gözlenen_uzunluk:\n                sayaç += 1\n        except ValueError: # birinci seçenek hiç gözlenmediyse\n            continue\n    return sayaç / deneme\nBirinci fonksiyon oylar_üret(r, oy_sayısı) iki adaylı bir seçimde, birinci adayın oy oranı r olacak şekilde oy_sayısı kadar rastgele oy üretir. İkinci fonksiyon ise, birincinin çıktısını alıp onun içinde ardışık değer zincirinin en büyük uzunluğunu bulur.\nÖrnek:\nrandom.seed(20232023)\ns = oylar_üret(r=0.2, oy_sayısı=10)\ns\n\n['KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'KK', 'RTE', 'RTE', 'KK']\nen_uzun_dizi_uzunluğu(s, \"RTE\")\n\n2\nÜçüncü fonksiyon olabilirlik, en uzun dizi uzunluğunun tam 2 olduğu durumların olasılığını, aynı oy oranı ve oy sayısıyla birçok rastgele sandık üreterek kestirir.\nÖrneğin, oy oranı 0.2 ise ve 10 adet oy açıldıysa, tam 2 uzunlukta en az bir dizi olması olasılığı yaklaşık 0.22 olur.\nolabilirlik(0.2, oy_sayısı=10, deneme=10000, gözlenen_uzunluk=2)\n\n0.2184\nTabii biz asıl oy oranını bilmiyoruz, o yüzden tersten gideceğiz: Farklı oy oranları için, 100 oy içinde tam 2 uzunlukta en az bir dizi görme olabilirliğini hesaplayacağız.\nCode\nr = np.linspace(0, 1, 101)\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nplt.plot(r, L)\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Olabilirlik\")\nplt.grid()\n\n\n\n\n\nOy oranına göre, iki uzunlukta en az bir dizi görme olasılığı\nBu sandıkta, RTE’nin oy oranının olasılık dağılımının 0.5’e doğru neredeyse sıfıra indiğini görüyoruz. Dağılımın tepesi 0.15 değerinde. Dağılım geniş bir aralığa yayılmış olduğu için sonuç hakkında net bir şey söylemek şimdilik zor olsa da, yarıyı geçme şansının çok düşük olduğu anlaşılıyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#bayes-formülü",
    "title": "Seçim Sandığında Bayes",
    "section": "Bayes formülü",
    "text": "Bayes formülü\nBu noktada biraz duraklayıp bu problemi bir bayesçi kestirim alıştırması olarak ifade edelim. Oy oranı \\(r\\) ise ve \\(G\\) gözlemimizi (yani en uzun ardışık zincirin 2 uzunlukta olmasını) temsil ediyorsa, bu gözlem verilmiş olarak oy oranının olasılık dağılımını Bayes formülüyle ifade ederiz:\n\\[P(r | G) = \\frac{1}{P(G)}P(G|r)P(r)\\]\nÖnsel inanç: Burada \\(P(r)\\), bir şey gözlemeden önceki oy oranının dağılımıdır. Hiç bir şey gözlemediysek bunu nasıl bilebiliriz? Geçmiş tecrübelerimize dayanarak bir fikrimiz olabilir, veya her şey olabilir diyerek 0-1 arasında düzgün bir dağılım varsayabiliriz. Bu dağılıma önsel inanç (“prior belief”) denir.\nOlabilirlik: \\(P(G|r)\\) ifadesi, belli bir oy oranı \\(r\\) ile, gözlemimizin olasılığını verir. Yukarıdaki olabilirlik fonksiyonu tam bunu yapar. Bu faktörün teknik adı da zaten olabilirlik (“likelihood”).\nOlabilirlik fonksiyonu ile sonuçların nasıl üretildiğine dair modelimizi analizin içine katarız. Mesela burada, iki seçenekten birinin rastgele seçildiği bir Bernoulli modeli kullandık. Başka problemlerde başka modeller kullanılması gerekecektir.\nOlabilirlik, aslında aradığımız şeyin tersidir: Parametrelerin bilinen değeriyle çıktılar üretir. Oysa biz bu çıktılardan yola çıkarak parametrenin ne olduğuna dair bir fikir edinmek istiyoruz. Yukarıdaki Bayes formülü bu ters problemi çözmemizi sağlar.\nNormalleştirme: \\(P(G)\\) ifadesi bir normalleştirme sabitidir ve şu şekilde hesaplanabilir: \\[P(G) = \\sum_{r} P(G|r)P(r)\\] Ama genellikle bu sabiti doğrudan kullanmaya gerek olmaz.\nSonsal inanç: Yukarıdaki faktörleri birleştirerek elde ettiğimiz \\(P(r|G)\\) olasılığına sonsal inanç (“posterior belief”) denir. Bu, önsel inancımızın eldeki veriyle güncellenerek düzeltilmiş halidir."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#düzgün-önsel-ile-tahminler",
    "title": "Seçim Sandığında Bayes",
    "section": "Düzgün önsel ile tahminler",
    "text": "Düzgün önsel ile tahminler\nŞimdi bunu problemimize uygulayalım. Düzgün dağılmış bir önsel alalım, yani RTE’nin bu sandıktaki oy oranının 0 ile 1 arasında eşit olasılıkla herhangi bir değerde olabileceğini düşünelim.\n\n\nCode\nr = np.linspace(0, 1, 101)\nprior = np.ones_like(r) \nprior /= sum(prior) # normalizasyon\n\n# her oy oranı için olabilirliği hesapla\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.grid()\n\n\n\n\n\nÖnsel (prior) dağılım ve 100 oyda an az bir tane 2 uzunlukta zincir olabilirliği ile sonsal (posterior) dağılım\n\n\n\n\nÖnceki grafiğin aynısı çıktı, ki sabit bir önsel aldığımız için böyle olması gerek (olabilirliği sabit bir sayıyla çarpıyoruz)\nBurada tek bir oy oranı tahmini çıkarmıyoruz. Bayesçi kestirim bize tek tahminler değil, tahmin edilecek değişken için bir olasılık dağılımı verir. İsterseniz bu dağılımdan tek tahminler (point estimate) çıkarabilirsiniz. Örneğin en yüksek olasılıklı değeri 0.15, ortalama değeri 0.17 olarak bulabiliriz.\nBaşka bir özet sayı, sonsal dağılımın ortasında %90 alanı kaplayan aralığın (“highest posterior density interval”) sınırlarıdır. Buna göre RTE’nin oy oranını, %90 olasılıkla (0.06, 0.29) arasında tahmin edebiliriz.\n\n\nCode\nprint(f\"En muhtemel değer: { r[np.argmax(posterior)] : .2f}\")\nprint(f\"Ortalama değer: { np.sum(r*posterior): .2f}\")\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\nEn muhtemel değer:  0.14\nOrtalama değer:  0.17\n%90 yoğunluk aralığı: 0.06  - 0.29"
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#güncelleme",
    "title": "Seçim Sandığında Bayes",
    "section": "Güncelleme",
    "text": "Güncelleme\nDiyelim 100 oy daha sayıldı ve RTE’ye yine en fazla 2 uzunlukta ardışık zincirler gözlediniz. Bu yeni veriyle RTE’nin oy oranına dair inancınızı (sonsal dağılımı) güncelleyebilirsiniz.\nBayesci analizin güzel tarafı, bunu yaparken sıfırdan başlamak zorunda olmamanız. Bir önceki adımda bulduğunuz sonsal dağılımı bu sefer önsel dağılım olarak kullanıp aynı işlemi tekrarlayabilirsiniz.\nOlabilirlik hesabını tekrarlamak zorunda değilsiniz, çünkü parametreler tamamen aynı olduğu için fonksiyon da aynı olacak.\n\n\nCode\nprior = posterior\n\n# her oy oranı için sonsal olasılığı hesapla\nposterior = L*prior\nposterior /= sum(posterior) # normalizasyon\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 200 oy\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.08  - 0.24\n\n\n\n\n\nYeni veri ile tahmin güncelleme\n\n\n\n\nBu yeni veri ile sonsal dağılım biraz daha daraldı. Güncellemeden sonra %90 yoğunluk aralığı 0.08-0.24 oldu."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-olabilirlikler",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı olabilirlikler",
    "text": "Farklı olabilirlikler\nSandıktaki oyların sayımı bittiğinde RTE’nin 352 oydan 83’ünü aldığı görüldü. Yani gerçekleşen oran 0.24 olmuş. Bu değer yukarıda bulduğumuz %90 aralığının tam sınırında. Sonsal dağılımımız olması gerekenin biraz altında kalmış görünüyor.\nBunun birkaç sebebi olabilir. Birincisi, ardışık oy dizisi uzunluğu iyi bir gösterge olmayabilir. İkincisi, benim gözlemim yanlış olabilir. Belki başlarda arka arkaya üç tane RTE oyu çıkmıştır da, ben o sırada ardışıklığa dikkat etmediğim için kaçırmış olabilirim.\nHesabı bu ihtimale göre iki aşamada tekrarlayalım. İlk 100 oy içinde en uzun RTE zinciri 3 uzunlukta olsun, ondan sonraki 100 oy içinde 2 uzunlukta olsun.\n\n\nCode\nprior1 = np.ones_like(r)\nprior1 /= sum(prior1)\n\nL1 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=3) for rr in r]\nposterior1 = L1*prior1\nposterior1 /= sum(posterior1) # normalizasyon\n\nprior2 = posterior1\nL2 = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior2 = L2*prior2\nposterior2 /= sum(posterior2) # normalizasyon\n\nplt.plot(r, prior1, label=\"prior 1\")\nplt.plot(r, posterior1, label=\"posterior 1 (prior 2)\")\nplt.plot(r, posterior2, label=\"posterior 2\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$\")\nplt.legend()\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior2))][0]}  - {r[(np.cumsum(posterior2)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.11  - 0.3\n\n\n\n\n\nİlk 100 oy içinde 3 uzunlukta en az bir zincir, ikinci 100 oy içinde 2 uzunlukta en az bir zincir gözlendiğinde sonsal dağılımlar\n\n\n\n\nİlk 100 oyda üçlü diziler olduğu için, birinci sonsal dağılımımız sağa kaymış, yani yüksek oy oranlarının olasılığı artmış. Bunun sonucu olarak da 200 oy gözlendikten sonraki sonsal, bir önceki çözümümüze göre daha sağa kaymış. Böylece %90 yoğunluk aralığı daha geniş ve gerçekleşen oranı içeriyor."
  },
  {
    "objectID": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "href": "posts/tr/2023/secim-sandiginda-bayes/index.html#farklı-önsel-geçmiş-seçimden-bilgi-aktarma",
    "title": "Seçim Sandığında Bayes",
    "section": "Farklı önsel: Geçmiş seçimden bilgi aktarma",
    "text": "Farklı önsel: Geçmiş seçimden bilgi aktarma\nSon olarak, farklı bir önsel kullanmayı deneyelim.\nŞimdiye kadar kullandığımız önsel, bir aday için bütün oy oranlarının eşit olasılıkta olduğunu varsayıyordu. Oysa bunun doğru olmadığını geçmiş tecrübemizden biliyoruz. Seçimin birinci turunda aynı sandıkta 357 oyun 81’i RTE’ye gittiğini gözlemiştik. Bu bilgiyle, Beta dağılımına uyan bir önsel belirleyebiliriz:\n\\[P(r) = C\\ r^{81} (1-r)^{276}\\]\nYine, ilk 100 oyda en uzun zincirin 2 uzunlukta olduğunu varsayarak sonsal hesaplayalım.\n\n\nCode\nprior = r**81 * (1-r)**276\nprior /= sum(prior)\n\nL = [olabilirlik(rr, oy_sayısı=100, deneme=10000, gözlenen_uzunluk=2) for rr in r]\nposterior = L*prior\nposterior /= sum(posterior)\n\nplt.plot(r, prior, label=\"prior\")\nplt.plot(r, posterior, label=\"posterior\")\nplt.xlabel(\"RTE oy oranı\")\nplt.ylabel(\"Yoğunluk\")\nplt.title(\"$P(r|G)$, 100 oy\")\nplt.legend()\nplt.xlim((0.1,0.4))\nplt.grid()\n\nprint(f\"%90 yoğunluk aralığı: {r[(0.05&lt;np.cumsum(posterior))][0]}  - {r[(np.cumsum(posterior)&lt;0.95)][-1]}\")\n\n\n%90 yoğunluk aralığı: 0.19  - 0.25\n\n\n\n\n\nGeçmiş seçimin oy oranlarıyla oluşturulan önsel ve bununla elde edilen sonsal dağılım.\n\n\n\n\n100 oy sayımında en fazla 2 uzunlukta zincir görmekle, sonsal olasılık dağılımının hafifçe sola kaydığını, yani oy oranının daha düşük ihtimallere kaydığını görüyoruz.\nAyrıca, daha belirli (informative) bir önsel seçmekle, sonsal dağılımımızın %90 yoğunluk aralığı daraldı, yani belirsizliği daha az bir sonuç elde ettik."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "",
    "text": "Prof. F. Curtis Michel (1934-2015) was the Andrew Hays Buchanan Professor of Astrophysics at Rice University. In a departmental seminar on March 3, 2005, he described a method for numerical solution of a differential equation of motion. He developed the method independently during his work on stellar magnetospheres, but he did not claim any originality. As far as I know, Prof. Michel did not publish this method. I recently found my notes from that that talk and I decided to write it up here."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#the-algorithm",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#the-algorithm",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "The algorithm",
    "text": "The algorithm\nLet \\(\\mathbf{x}\\in \\mathbb{R}^3\\) be the position and \\(\\mathbf{v}\\in \\mathbb{R}^3\\) be the velocity of a particle at a given time. The time evolution of these variables obey the following equation of motion:\n\\[\n\\begin{eqnarray}\n\\dot{\\mathbf{x}} &=& \\mathbf{v}\\\\\n\\dot{\\mathbf{v}} &=& \\mathbf{a}(\\mathbf{x},\\mathbf{v})\n\\end{eqnarray}\n\\]\nwhere the acceleration \\(\\mathbf{a}(\\mathbf{x},\\mathbf{v})\\) is a given function of position and velocity.\nSuppose at time step \\(t_n\\) the position \\(\\mathbf{x}_n\\) and velocity \\(\\mathbf{v}_n\\) are given, and we want to estimate the \\(\\mathbf{x}_{n+1}\\) and velocity \\(\\mathbf{v}_{n+1}\\) at the next time step \\(t_{n+1}\\equiv t_n+\\tau\\). The scheme proposed by Michel is as follows:\nDefine an intermediate step \\(\\mathbf{x}_{n+\\textonehalf}\\):\n\\[\n\\mathbf{x}_{n+\\textonehalf} \\equiv \\mathbf{x}_{n} + \\frac{\\tau}{2}\\mathbf{v}_{n}\n\\]\nThen, the velocity is updated as:\n\\[\n\\begin{eqnarray}\n\\mathbf{v}_{n+1} = \\mathbf{v}_{n} + \\tau \\mathbf{a}\\left(\\mathbf{x}_{n+\\textonehalf}, \\frac{1}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n})\\right)\n\\end{eqnarray}\n\\]\nand the position is updated as:\n\\[\n\\begin{eqnarray}\n\\mathbf{x}_{n+1} &=& \\mathbf{x}_{n+\\textonehalf} + \\frac{\\tau}{2}\\mathbf{v}_{n+1} \\\\\n&=& \\mathbf{x}_{n} + \\frac{\\tau}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n})\n\\end{eqnarray}\n\\]\nHere is a representation of the scheme: \nThe update equations have \\(\\mathbf{v}_{n+1}\\) on the right-hand side, making this an implicit scheme. Implicit schemes have the additional burden of solving the equations for the values at step \\(n+1\\); however, they are generally more accurate and stable."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-motion-under-magnetic-field",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-motion-under-magnetic-field",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Application: Motion under magnetic field",
    "text": "Application: Motion under magnetic field\nA particular form of velocity-dependent acceleration is provided by the magnetic field:\n\\[\n\\mathbf{a}(\\mathbf{x},\\mathbf{v}) = \\frac{q}{m}\\mathbf{v}\\times\\mathbf{B}(\\mathbf{x})\n\\]\nwhere \\(q\\) and \\(m\\) are the electric charge and mass of the accelerated particle, respectively. Because of the cross product, the acceleration is always perpendicular to the velocity. Therefore, the speed and the kinetic energy of the particle is constant in time.\nThe velocity update equation becomes:\n\\[\n\\mathbf{v}_{n+1} = \\mathbf{v}_{n} + \\frac{\\tau q}{2m}\\left(\\mathbf{v}_{n+1}+\\mathbf{v}_{n}\\right)\\times \\mathbf{B}\\left(\\mathbf{x}_n + \\frac{\\tau}{2}\\mathbf{v}_n\\right)\n\\]\nIn order to solve for \\(\\mathbf{v}_{n+1}\\), we write the cross product as a matrix-vector product: Let \\(\\mathbf{v} = [v_x, v_y, v_z]^\\intercal\\) and \\(\\mathbf{B} = [B_x, B_y, B_z]^\\intercal\\). Then\n\\[\n\\mathbf{v}\\times\\mathbf{B} = \\begin{bmatrix}\n0 & B_z & -B_y \\\\\n-B_z & 0 & B_x \\\\\nB_y & -B_x & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nv_x \\\\\nv_y \\\\\nv_z\n\\end{bmatrix} := \\mathbb{B}\\mathbf{v}\n\\]\nUsing the matrix \\(\\mathbb{B}_n\\) constructed from \\(\\mathbf{B}\\left(\\mathbf{x}_n + \\frac{\\tau}{2}\\mathbf{v}_n\\right)\\), the velocity update equation can be written as:\n\\[\n\\mathbf{v}_{n+1} = \\left(\\mathbb{I}+\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n + \\frac{\\tau q}{2m}\\mathbb{B}_n\\mathbf{v}_{n+1}\n\\] where \\(\\mathbb{I}\\) is the 3-by-3 identity matrix. Now we can collect \\(\\mathbf{v}_{n+1}\\) terms on the left: \\[\n\\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_{n+1} = \\left(\\mathbb{I}+\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n\n\\]\nIt is straightforward to show that the determinant \\[\\det \\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right) = \\left(\\frac{q\\tau}{2m}\\right)^2\\left(B_x^2+B_y^2+B_z^2\\right)\\] is nonzero as long as there is a magnetic field present. Then, the matrix is invertible and the next step velocity can be solved as:\n\\[ \\mathbf{v}_{n+1} = \\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)^{-1}\\left(\\mathbb{I}+ \\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\\mathbf{v}_n\\]\nThe position vector is then updated using the current and next velocity vectors:\n\\[ \\mathbf{x}_{n+1} = \\mathbf{x}_{n} + \\frac{\\tau}{2}(\\mathbf{v}_{n+1} + \\mathbf{v}_{n}) \\]"
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#kinetic-energy-preservation",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#kinetic-energy-preservation",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Kinetic energy preservation",
    "text": "Kinetic energy preservation\nPhysically, a magnetic field does not change the kinetic energy of the particle. Its acceleration serves only to deflect the trajectory sideways. To see this, consider the work done on the particle by the magnetic field force\n\\[\\begin{eqnarray}\nW &=& \\int \\mathbf{F}(\\mathbf{x})\\mathrm{d}\\mathbf{x}\\\\\n&=& \\int \\mathbf{F}(\\mathbf{x})\\cdot\\mathbf{v}\\mathrm{d}t\\\\\n&=& \\int (q\\mathbf{v}\\times \\mathbf{B})\\cdot\\mathbf{v}\\mathrm{d}t\n\\end{eqnarray}\n\\] The integrand is identically zero, because the force is prependicular to the velocity at all times. There is no work done on the particle, so its kinetic energy stays constant.\nMichel’s method has the merit of perfectly preserving the kinetic energy under a magnetic field. Consider the velocity update formula:\n\\[\n\\mathbf{v}_{n+1} = \\mathbb{M}\\mathbf{v}_n\n\\] where \\[\n\\mathbb{M}:=\\left(\\mathbb{I}-\\frac{\\tau q}{2m}\\mathbb{B}_n\\right)^{-1}\\left(\\mathbb{I}+ \\frac{\\tau q}{2m}\\mathbb{B}_n\\right)\n\\]\nBy explicit calculation we can show that the matrix \\(\\mathbb{M}\\) is orthogonal, that is: \\[\n\\mathbb{M}\\mathbb{M}^\\intercal=\\mathbb{I},\n\\]\nOrthogonal matrices transform vectors without changing their length. To see this, multiply both sides of the velocity update formula with \\(\\mathbf{v}_{n+1}^\\intercal\\).\n\\[\n\\begin{eqnarray}\n\\mathbf{v}_{n+1}^\\intercal\\mathbf{v}_{n+1} &=& \\mathbf{v}_{n+1}^\\intercal\\mathbb{M}\\mathbf{v}_n\\\\\n\\mathbf{v}_{n+1}^\\intercal\\mathbf{v}_{n+1} &=& \\mathbf{v}_n^\\intercal\\mathbb{M}^\\intercal\\mathbb{M}\\mathbf{v}_n\\\\\n|\\mathbf{v}_{n+1}|^2 &=& \\mathbf{v}_n^\\intercal\\mathbb{I}\\mathbf{v}_n\\\\\n|\\mathbf{v}_{n+1}|^2 &=& |\\mathbf{v}_{n}|^2\\\\\n\\end{eqnarray}\n\\]"
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#simulation-of-a-charged-particle-in-a-dipolar-magnetic-field",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#simulation-of-a-charged-particle-in-a-dipolar-magnetic-field",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Simulation of a charged particle in a dipolar magnetic field",
    "text": "Simulation of a charged particle in a dipolar magnetic field\nTo illustrate the method, we will trace the path of a charged test particle under the field of a magnetic dipole fixed at the origin. The dipole moment is set to be in the negative-\\(\\mathbf{z}\\) direction, imitating the Earth’s magnetic field.\nThen, the magnetic field vector in cartesian coordinates is:\n\\[\n\\mathbf{B}(\\mathbf{x}) = -\\frac{B_0}{r^5}\n\\begin{bmatrix}\n3xz \\\\\n3yz \\\\\n3z^2-r^2\n\\end{bmatrix}\n\\] where \\(\\mathbf{x}=[x,y,z]^\\intercal\\) and \\(r^2 = x^2+y^2+z^2\\). The coefficient \\(B_0\\) specifies the field strength at unit distance on the equatorial plane (\\(z=0\\) and \\(r=1\\)).\nLet’s write a program to implement the schema for this particular field:\n\nimport numpy as np\nimport matplotlib.pylab as plt\n\nq = -1 # particle charge\nm = 1  # particle mass\nB0 = 100 # field strength at unit distance on the z=0 plane\n\ndef dipole_field(x):\n    # Field of a magnetic dipole located at origin, pointing to the negative-z direction\n    # North pole is up, south pole is down.\n    # x  : Three-element array of position\n    # B0 : field strength at unit distance from origin\n    rsq = (x**2).sum()\n    return -B0 / (rsq**(5/2)) * np.array([3*x[0]*x[2], 3*x[1]*x[2], (3*x[2]**2-rsq)])\n\ndef update(x, v, dt, field):\n    # the half-step\n    x1 = x + v*dt/2\n    # Magnetic field vector at the half-step\n    B = field(x1)\n    # Magnetic field matrix for cross-product\n    B_mat = np.array([\n        [0, B[2], -B[1]],\n        [-B[2], 0, B[0]],\n        [B[1], -B[0], 0]\n        ])\n    # the left matrix\n    M1 = np.identity(3) - dt*q/(2*m)*B_mat\n    # the right matrix\n    M2 = np.identity(3) + dt*q/(2*m)*B_mat\n    # Solve for the next step velocity\n    v_next = np.linalg.solve(M1, np.dot(M2, v))\n    # Determine the next step position\n    x_next = x + (v_next + v)*dt/2\n\n    return x_next, v_next\n\nNote that the function update() can be used with any magnetic field that is defined as a function, as shown above.\nWe initialize the particle at \\(\\mathbf{x} = [3, 0, 0]^\\intercal\\) with velocity \\(\\mathbf{v} = [0, 0.4, 0.5]^\\intercal\\), and follow it for 3000 steps, with time step 0.1\n\nx = np.array([3,0,0])\nv = np.array([0,0.4,0.5])\ndt = 0.1\n\nt_steps = np.arange(0,3000*dt,dt)\n# store intermediate steps:\nx_steps = [x]\nv_steps = [v]\nfor t in t_steps:\n    x, v = update(x,v,dt,dipole_field)\n    x_steps.append(x)\n    v_steps.append(v)\n\nThe resulting path of the particle can then be visualized:\n\nx_t = [_[0] for _ in x_steps]\ny_t = [_[1] for _ in x_steps]\nz_t = [_[2] for _ in x_steps]\nax = plt.figure().add_subplot(projection='3d')\nax.plot(x_t, y_t, z_t)\nax.scatter(x_t[0], y_t[0], z_t[0], c=\"C1\", s=10)\nax.set_aspect(\"equal\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\nThe orange dot on the right side shows the initial position of the particle. The particle follows the familiar cyclotron, bounce, and drift modes as it moves under the influence of the dipole field. This path simulates an energetic electron in the radiation belt region of the Earth’s magnetosphere.\nThe kinetic energy of a charged particle moving under a purely magnetic field must stay constant. We can use this fact to check the stability of the scheme.\n\nspeed = [np.sqrt((_**2).sum()) for _ in v_steps]\nprint(\"Range of speed:\", max(speed) - min(speed))\nplt.plot(speed)\nplt.grid()\nplt.xlabel(\"Steps\")\nplt.ylabel(\"Speed\");\n\nRange of speed: 1.0325074129013956e-14\n\n\n\n\n\nThe speed (magnitude of velocity) is extremely stable, as expected, despite the fact that the direction of velocity changes rapidly due to the system’s dynamics. The difference between largest and smallest values is comparable to the machine precision."
  },
  {
    "objectID": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-gravitational-acceleration",
    "href": "posts/en/2024/Michels-method-for-numerical-solution-of-eom/index.html#application-gravitational-acceleration",
    "title": "Michel’s method for numerical solution of equations of motion",
    "section": "Application: Gravitational acceleration",
    "text": "Application: Gravitational acceleration\nHere we apply the method to the Kepler problem, 2-dimensional motion of a test body under the influence of a large mass. Here, the acceleration does not depend on velocity, only to position, so the update step is simpler.\nNote that when the acceleration does not depend on the velocity, the scheme is not implicit anymore. The position and velocity updates can be combined in the following form:\n\\[\n\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\tau\\mathbf{v}_n + \\frac{\\tau^2}{2} \\mathbf{a}(\\mathbf{x}_n+\\frac{\\tau}{2}\\mathbf{v}_n)\n\\]\nThe gravitational acceleration of a body due to a mass \\(M\\) is\n\\[\n\\mathbf{a}(\\mathbf{x}) = -\\frac{GM}{|\\mathbf{x}|^3}\\mathbf{x}\n\\]\nwhere \\(G\\) is the gravitational constant. In the simulation, we will set \\(GM=10\\)1. The attracting mass is fixed at the origin.1 Physically this corresponds to a very small asteroid, with a mass of about 150 million tonnes and a diameter of 100-200 meters. With this choice we have convenient position and velocity values of the order of 1.\n\ndef update(x, v, dt, acceleration):\n    x1 = x + v*dt/2 # intermediate step\n    v_next = v + dt*acceleration(x1)\n    x_next = x + (v_next + v)*dt/2\n    return x_next, v_next\n\nGM = 10 # acceleration factor\ndef grav_acc(x):\n    rcube = ((x**2).sum())**(3/2)\n    return -GM/rcube * x\n\ndef iterate(x0, v0, dt, nsteps):\n    x = x0; v = v0;\n    x_steps = [x]\n    v_steps = [v]\n    t_steps = np.arange(0,nsteps*dt+dt,dt)\n    for step in range(nsteps):\n        x, v = update(x,v,dt,grav_acc)\n        x_steps.append(x)\n        v_steps.append(v)\n    x_steps = np.array(x_steps)\n    v_steps = np.array(v_steps)\n    return t_steps, x_steps, v_steps\n\nInitialize the position and velocity, set a time step of 0.2 seconds, and follow for 500 steps.\n\nx0 = np.array([4,0]) # initial position\nv0 = np.array([0,1.8])  # initial velocity\nt, x, v = iterate(x0, v0, dt=0.2, nsteps=500)\n\nVisualize the trajectory. The green arrow is the initial velocity, and the star at the origin indicates the attracting body.\n\nplt.plot(x[:,0], x[:,1], c=\"C1\")\nplt.arrow(x[0,0], x[0,1], dx=v[0,0], dy=v[0,1], width=0.1, ec=\"C2\", fc=\"C2\", head_width=0.5)\nplt.scatter(0,0,s=200,marker=\"*\",c=\"k\")\nplt.grid(True)\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.gca().set_aspect(\"equal\")\n\n\n\n\nUnlike the magnetic force, gravitational force does not keep the kinetic energy constant. However, energy conservation still applies. The sum of kinetic energy and the potential energy per mass should be constant:\n\\[\nE = \\frac{1}{2}|\\mathbf{v}|^2 - \\frac{GM}{|\\mathbf{x}|} = \\mathrm{constant}\n\\]\nChecking the energy from numerical solution tells us about the error of the algorithm:\n\ndef total_energy(x,v):\n    ke = 0.5*(v**2).sum(axis=1)\n    r = np.sqrt((x**2).sum(axis=1))\n    pe = -GM/r\n    return ke+pe\n\nplt.plot(t, total_energy(x,v))\nplt.title(\"Total energy per mass\")\nplt.xlabel(\"time\")\nplt.ylabel(\"Energy\")\nplt.grid();\n\n\n\n\nThe energy oscillates with a small amplitude, but there is no significant drift. With a smaller step size, the oscillations can be reduced:\n\nx0 = np.array([4,0]) # initial position\nv0 = np.array([0,1.8])  # initial velocity\n\nt, x, v = iterate(x0, v0, dt=0.2, nsteps=500)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.2, $\\\\Delta E=${delta_e:.2e}\")\n\nt, x, v = iterate(x0, v0, dt=0.1, nsteps=1000)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.1, $\\\\Delta E=${delta_e:.2e}\")\n\nt, x, v = iterate(x0, v0, dt=0.05, nsteps=2000)\ne = total_energy(x,v)\ndelta_e = max(e)-min(e)\nplt.plot(t, e, label=f\"dt=0.05, $\\\\Delta E=${delta_e:.2e}\")\n\nplt.title(\"Total energy per mass\")\nplt.xlabel(\"time\")\nplt.ylabel(\"Energy\")\nplt.grid()\nplt.legend();\n\n\n\n\nFor every halving of the time step, the amplitude of oscillation \\(\\Delta E\\) is reduced by a factor of 4. Even though we did not do a formal analysis, this observation suggests that Michel’s method is a second-order scheme, where the error reduces as the square of the step size: \\(\\epsilon \\sim \\mathcal{O}(\\tau^2)\\)"
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "",
    "text": "Python’la yapay öğrenme modelleri çalıştıran her veri bilimcinin aşina olduğu bir kütüphanedir scikit-learn. Son derece zengin bir algoritma koleksiyonunu barındırır. Çoğu zaman o algoritmaları olduğu gibi kullanmak yeterlidir.\nAma bazen çok özelleşmiş bir algoritmaya ihtiyaç duyabilirsiniz. Bunu scikit-learn arayüzüne uygun şekilde yazarsanız, kütüphanenin sağladığı birçok başka kolaylığa da rahatça erişebilirsiniz.\nBu yazıda, kendi regresyon veya sınıflandırıcı modelinizi nasıl scikit-learn modülünün parçasıymış gibi yazabileceğinizi anlatacağım. Önce, scikit-learn sisteminin nasıl kullanıldığına bir bakalım."
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#hareketli-ortalama",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#hareketli-ortalama",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Hareketli ortalama",
    "text": "Hareketli ortalama\nÖrnek olarak, verilerin hareketli ortalamasını veren bir dönüştürücü hazırlayalım. Böyle bir işlem için bir dönüştürücü yazmak aşırı gelebilir, ama yararlı olacağı durumlar vardır. Sözgelişi, modelin eğitim kümesini hazırlarken bir adımda hareketli ortalama alıyor olabiliriz. Önceden bu işlemi bir dönüştürücü haline getirirsek, bütün eğitim ve kestirim sürecini bir pipeline içine koymamız, çapraz doğrulamaları aksamadan yapmamız mümkün olur.\n\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.utils.validation import check_is_fitted\n\nclass RollingMean(BaseEstimator, TransformerMixin):\n    def __init__(self, n=15):\n        self.n = n\n    \n    def moving_average_(self, a):\n        return np.convolve(a, np.ones(self.n), \"valid\") / self.n\n    \n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        return self\n    \n    def transform(self, X):\n        check_is_fitted(self, 'n_features_in_')\n        X = check_array(X)\n        # Moving average will have len(X) - n + 1 rows. For consistent size, fill the first n-1 rows with NaN.\n        empty_rows = np.array([np.nan]*(X.shape[1]*(self.n-1))).reshape(self.n-1,X.shape[1])\n        movav = np.apply_along_axis(self.moving_average_, axis=0, arr=X)\n        return np.r_[empty_rows, movav]\n\nBu örnekte fit() herhangi bir işlem yapmıyor, sadece girdi için doğruluk denetimi yapıyor ve n_features_in_ nesne özelliğini yaratıyor. Daha sonra transform() çağrıldığında bu değişkenin var olup olmadığı denetleniyor.\nÜç değişkenli bir rastgele yürüyüş verisi üretelim:\n\nnp.random.seed(29101923)\nX = np.cumsum(np.random.rand(30).reshape(3,-1).T, axis=0)\nX\n\narray([[0.74977745, 0.96371498, 0.32437245],\n       [1.71830174, 1.4121694 , 0.50916844],\n       [2.10912041, 1.54142327, 0.77301929],\n       [2.12452587, 1.97102509, 1.23124119],\n       [2.34991983, 2.00229673, 1.87464155],\n       [2.75549879, 2.71351979, 2.71059991],\n       [2.87756103, 3.29451338, 3.27770838],\n       [3.64506605, 3.47371139, 3.9280419 ],\n       [4.18478839, 4.08826894, 4.22898858],\n       [5.0129616 , 4.10754193, 4.32410028]])\n\n\n\nRollingMean(n=3).fit(X).transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])\n\n\nBu örnekte fit() ayrı bir iş yapmadığı için, aynı işlem doğrudan fit_transform() ile de yapılabilir. Bu metot TransformerMixin sınıfından miras alınır, o yüzden bizim açıkça eklememize lüzum yok.\n\nRollingMean(n=3).fit_transform(X)\n\narray([[       nan,        nan,        nan],\n       [       nan,        nan,        nan],\n       [1.5257332 , 1.30576922, 0.53552006],\n       [1.98398267, 1.64153925, 0.83780964],\n       [2.19452204, 1.83824836, 1.29296734],\n       [2.40998149, 2.2289472 , 1.93882755],\n       [2.66099322, 2.67010996, 2.62098328],\n       [3.09270862, 3.16058152, 3.30545006],\n       [3.56913849, 3.61883124, 3.81157962],\n       [4.28093868, 3.88984076, 4.16037692]])"
  },
  {
    "objectID": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#minimum-maksimum-ölçekleme",
    "href": "posts/tr/2023/scikit-tahminleyici-yazmak/index.html#minimum-maksimum-ölçekleme",
    "title": "Scikit-learn ile Kendi Tahminleyicilerimizi Yazmak",
    "section": "Minimum-maksimum ölçekleme",
    "text": "Minimum-maksimum ölçekleme\nBaşka bir örnek olarak, minimum-maksimum arası ölçekleme için bir dönüştürücü oluşturalım1. Bu örnekte fit() metodu boş durmuyor, eğitim kümesinin en büyük ve en küçük değerlerini bulup bir kenara yazıyor.1 Bunun için hazır bir dönüştürücü var ama örnek için yokmuş gibi yapalım.\n\nclass MinmaxScaler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, X, y=None):\n        X = check_array(X)\n        self.n_features_in_ = X.shape[1]\n        self.max_ = X.max(axis=0)\n        self.min_ = X.min(axis=0)\n        return self\n    def transform(self, X):\n        check_is_fitted(self, \"max_\")\n        X = check_array(X)\n        return (X - self.min_) / (self.max_ - self.min_)\n\nBurada ölçekleme için herhangi bir parametre alınmadığından başlatıcı __init__ boş kalıyor (ama Python sentaksı gereği sınıf tanımında bulunmak zorunda).\nBu dönüştürücünün bir öncekinden farkı, eğitim kümesine bağlı oluşu. Bu ölçekleyici, eğitim verisinin sütunlarının minimum ve maksimum değerlerini belirliyor. transform() işleminde ise, aldığı verileri minimum değer 0 ve maksimum değer 1 olacak şekilde lineer bir fonksiyonla dönüştürüyor.\nBu dönüştürücüyü yine rastgele üretilmiş verilerle deneyelim:\n\nX_train = np.random.rand(20,3)\nX_test = np.random.rand(10,3)\nX_test\n\narray([[0.35628925, 0.66850959, 0.84267923],\n       [0.69403501, 0.38350617, 0.62806853],\n       [0.38789683, 0.53202186, 0.61269947],\n       [0.51598942, 0.37764728, 0.36217848],\n       [0.02539575, 0.26381172, 0.64366218],\n       [0.91755454, 0.36221054, 0.50155528],\n       [0.09683517, 0.71776499, 0.7394225 ],\n       [0.81163167, 0.49544736, 0.63947337],\n       [0.58945327, 0.50590908, 0.03767375],\n       [0.09311029, 0.12597857, 0.72295531]])\n\n\n\nscaler = MinmaxScaler().fit(X_train)\nscaler.transform(X_test)\n\narray([[ 3.42904840e-01,  6.06674521e-01,  8.79083858e-01],\n       [ 7.17503869e-01,  2.46825035e-01,  6.44693329e-01],\n       [ 3.77961291e-01,  4.34343120e-01,  6.27907771e-01],\n       [ 5.20030762e-01,  2.39427514e-01,  3.54297244e-01],\n       [-2.40942497e-02,  9.56970773e-02,  6.61724192e-01],\n       [ 9.65412838e-01,  2.19936867e-01,  5.06519853e-01],\n       [ 5.51403047e-02,  6.68865111e-01,  7.66310361e-01],\n       [ 8.47932141e-01,  3.88163622e-01,  6.57149313e-01],\n       [ 6.01510650e-01,  4.01372740e-01, -1.15815402e-04],\n       [ 5.10089822e-02, -7.83330700e-02,  7.48325452e-01]])\n\n\nDönüştürücümüz check_estimator testlerinden de başarıyla geçiyor.\n\ncheck_estimator(MinmaxScaler())"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kaan Öztürk",
    "section": "",
    "text": "I’m a data scientist based in Istanbul and Tekirdağ, where I share my life with my family and our beloved cats.\nMy training is in physics, culminating with a PhD in magnetospheric physics. For over a decade, I taught at various universities, designing and leading programming courses and guiding students and teaching assistants alike. Eventually, I transitioned from academia to pursue an exciting career in data science.\nI’m a proud alumnus of İstanbul Erkek Lisesi (1991), Boğaziçi University (B.Sc. 1996, M.Sc 1999), and Rice University (PhD 2004).\nThis site is mainly about my technical notes. For the interested, I have written a number of pieces in other websites (all in Turkish):\n\nYalansavar: Scientific skepticism\nAçık Bilim: Popular science\nVeri Defteri: Data science, machine learning, programming.\nBirGün Gazetesi: Biweekly column on scientific skepticism\nWordpress Blog: My other personal blog"
  }
]